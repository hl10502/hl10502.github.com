<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[招标网站爬虫项目分析]]></title>
      <url>%2F2017%2F03%2F23%2Fscrapy-site-1%2F</url>
      <content type="text"><![CDATA[招标网站爬虫项目采用Scrapy框架，每天定时自动爬取当天通过检索关键字的招标信息。 需求由于市场与销售部门需要及时获知运营商、政府采购等相关行业的招标信息，参与招投标。故提出通过关键字检索，每天以Email的形式发送相关网站的招投标网页链接信息的需求。 Scrapy介绍Scrapy是Python语言开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。 使用Scrapy可以满足招标网站爬取的需求。 Scrapy主要包括了以下组件： 引擎(Scrapy Engine)用来处理整个系统的数据流处理, 触发事务(框架核心)。 调度器(Scheduler)用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址。 下载器(Downloader)用于下载网页内容, 并将网页内容返回给Spiders(Scrapy下载器是建立在twisted这个高效的异步模型上的)。 爬虫(Spiders)爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面。 项目管道(Item Pipeline)负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件(Downloader Middlewares)位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。 爬虫中间件(Spider Middlewares)介于Scrapy引擎和爬虫之间的框架，主要工作是处理爬虫的响应输入和请求输出。 调度中间件(Scheduler Middewares)介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 Scrapy爬取流程Scrapy运行流程大概如下： 引擎从调度器中取出一个链接(URL)用于接下来的抓取 引擎把URL封装成一个请求(Request)传给下载器 下载器把资源下载下来，并封装成应答包(Response) 爬虫解析Response 解析出实体（Item）,则交给实体管道进行进一步的处理 解析出的是链接（URL）,则把URL交给调度器等待抓取 招标网站爬虫实现招标网站爬虫实现过程如下： crontab每天23:00定时执行启动Scrapy 定义10个Spider，分别爬取各个网站当天发布的信息 在Pipeline中通过关键字过滤处理数据，并将数据分别写入到Spider的文本文件中存储 所有Spider爬取完成之后，读取所有的文本文件信息作为Email的内容，发送Email 创建scrapy项目在/opt目录下，创建scrapy_site项目1234567[root@node155 opt]# scrapy startproject scrapy_siteNew Scrapy project 'scrapy_site', using template directory '/usr/lib64/python2.7/site-packages/scrapy/templates/project', created in: /opt/scrapy_siteYou can start your first spider with: cd scrapy_site scrapy genspider example example.com 创建scrapy_site项目，自动生成以下目录文件 scrapy_site/ scrapy.cfg scrapy_site/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... 这些文件分别是： scrapy.cfg: 项目的配置文件 scrapy_site/: 该项目的python模块 scrapy_site/items.py: 项目中的item文件 scrapy_site/pipelines.py: 项目中的pipelines文件 scrapy_site/settings.py: 项目的设置文件 scrapy_site/spiders/: 放置spider代码的目录 定义配置文件 email.conf:配置Email地址信息 格式为spidername1,spidername2,…===reciveremail1,reciveremail2,…===creciveremail1,creciveremail2,… 例如：b2b10086,bidding===xxx@126.com,yyy@126.com===zzz@qq.com 有10个spider，最多可配置将10个spider的信息发送email keyword.conf：配置检索的关键字信息 格式为spidernamexxx===关键字1,关键字2,… 例如：b2b10086===云数据,云计算 定义Item定义Item的字段：文章标题、链接、描述、发布时间12345class SiteItem(scrapy.Item): title = scrapy.Field() link = scrapy.Field() desc = scrapy.Field() pubtime = scrapy.Field() 定义pipelines定义事件，初始化获取关键字、数据存储文件等，1234567891011121314151617def __init__(self): dispatcher.connect(self.spider_opended, signals.spider_opened) dispatcher.connect(self.spider_closed, signals.spider_closed) dispatcher.connect(self.engine_stopped, signals.engine_stopped) dispatcher.connect(self.engine_started, signals.engine_started) # 获取当前目录，当前目录为scrapy_site项目的根目录 self.curpath = os.getcwd() #爬虫爬取的数据存储文件目录 self.spidername_filepath = self.curpath + "/scrapy_site/msg/" # 从配置文件keyword.conf获取关键字字典值 self.keywordsDict = dict() self.getKeywords() # 爬取信息 self.msgDict = dict() 处理Item数据，将爬取到的数据过滤，放到msgDict字典中存储12345678910111213141516171819202122232425def process_item(self, item, spider): if not item['pubtime'] or not item['title'] : return item # 去除换行与空格及[] pubtime = item['pubtime'].encode(const.ENCODE) title = item['title'].encode(const.ENCODE) if self.checkTilte(self.keywordsDict.get(spider.name), title) and date.get_curdate() == pubtime: msgArr = self.msgDict.get(spider.name) if msgArr is None: msgArr = &#123;&#125; msgArr['id'] = 0 msgArr['msg'] = "" msgArr['id'] += 1 msgArr['msg'] += str(msgArr['id']) msgArr['msg'] += '---' msgArr['msg'] += item['title'].encode(const.ENCODE) msgArr['msg'] += '---' msgArr['msg'] += item['link'].encode(const.ENCODE) msgArr['msg'] += '\n' print("msgArr['msg']=" + msgArr['msg']) self.msgDict.setdefault(spider.name, msgArr) return item 爬取完成之后，将数据存储到该spider文件中123456789def spider_closed(self, spider): file = open(self.spidername_filepath + spider.name, 'w') #将爬取信息写入爬虫文件中 if self.msgDict : msg = self.msgDict.get(spider.name) if msg : message = msg['msg'] file.write(message) file.close() 主程序run.py定义run.py文件作为scrapy_site项目的入口。 通过CrawlerRunner来运行所有的spider12345678910111213141516171819......settings = get_project_settings()configure_logging(settings)runner = CrawlerRunner(settings)# 运行所有的spiderfor spider_name in runner.spider_loader.list(): runner.crawl(spider_name)d = runner.join()d.addBoth(lambda _: reactor.stop())# 阻塞直到所有的spider完成reactor.run() # 发送Email...... 发送Email使用smtplib发送Email123456789101112131415161718192021222324252627282930313233343536def sendMail(receiver, creceiver): receivers = (receiver + ',' + creceiver).split(',') loginfo = "收件人：" + receiver + ",Cc：" + creceiver print loginfo _log.info(loginfo) temp = getMsgByEmail(receiver) mail_msg = temp[0] isHasHref = temp[1] message = MIMEText(mail_msg, 'html', 'utf-8') message['From'] = const.SENDER message['To'] = receiver # 如果邮件中含有链接信息则发送给收件人 if isHasHref: message.add_header('Cc', creceiver) else: message.add_header('Cc', const.SENDER) # 邮箱主题 subject = '招标网站最新信息' + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) message['Subject'] = Header(subject, 'utf-8') try: smtpObj = smtplib.SMTP() smtpObj.connect(const.MAIL_HOST, 25) # 25 为 SMTP 端口号 smtpObj.login(const.MAIL_USER, const.MAIL_PASS) if isHasHref: smtpObj.sendmail(const.SENDER, receivers, message.as_string()) # 发送邮箱、接收邮箱、邮件内容 else: smtpObj.sendmail(const.SENDER, const.SENDER, message.as_string()) # 发送邮箱、接收邮箱、邮件内容 print "邮件发送成功" except smtplib.SMTPException, e: _log.error('无法发送邮件:' + str(e)) print e print "Error: 无法发送邮件" 定义spider前面是基础的框架，不涉及具体的网站的爬取。 定义10个spider分别爬取： b2b10086_spider.py bidding_spider.py chinabidding_spider.py chinaunicombidding_spider.py csbidding_spider.py gdgpogov_spider.py gzsggzyjyzx_spider.py mssportal_spider.py telewiki_spider.py zycggov_spider.py 后续如果需要增加爬取网站，只需增加爬取网站的spider，修改email.conf与keyword.conf配置文件。 定时启动scrapy使用crontab定时启动scrapy_site10 23 * * * /opt/scrapy_site/scrapy_start.sh scrapy_start.sh文件内容123[root@node155 scrapy_site]# cat scrapy_start.shcd /opt/scrapy_site/python scrapy_site/run.py]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改dom0的内存]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-6%2F</url>
      <content type="text"><![CDATA[增大dom0的内存增大dom0的内存为4G，修改/boot/grub/grub.cfg或者/etc/grub.cfg文件，将“dom0_mem=1752M,max:1752M”修改为“dom0_mem=4096M,max:4096M” 1[root@node0001 ~]# vi /boot/grub/grub.cfg 重启主机1[root@node0001 ~]# reboot]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[虚拟机绑定物理CPU]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-5%2F</url>
      <content type="text"><![CDATA[VM绑定物理CPU设置VM使用物理CPU排序中的第1,2个CPU1[root@node200 ~]# xe vm-param-set uuid=247e8ddf-9361-4557-889c-f6c1961c6706 VCPUs-params:mask=1,2 设置VM中CPU的权重设置VCPU的权重为512。1[root@node200 ~]# xe vm-param-set uuid=247e8ddf-9361-4557-889c-f6c1961c6706 VCPUs-params:weight=512 默认dom0权重为256，权重决定VM在CPU时间片中所占比例，权重范围从0到65535。 设置VM最大CPU使用率设置VM最大可以使用的CPU为单个CPU的80%。1[root@node200 ~]# xe vm-param-set uuid=247e8ddf-9361-4557-889c-f6c1961c6706 VCPUs-params:cap=80 设置为100，则使用1个物理CPU；设置为80，则只能使用一个物理CPU的80%性能；设置为400，则表示最大可以使用4个物理CPU。默认为0，不限制。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[虚拟机随物理主机开机自动启动]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-4%2F</url>
      <content type="text"><![CDATA[XenServer主机上的虚拟机默认是不会随物理主机开机自动启动的，是为了防止在HA环境中出现某些异常，所以物理主机异常断电恢复启动后，所有的虚拟机均为关机状态。但如果没配置HA，还是可以设置让虚拟机随物理主机开机自动启动。 设置pool的自动启动查看Pool的UUID123456[root@node200 ~]# xe pool-listuuid ( RO) : 53e9df09-bb99-3014-ffdf-3a781a9a84d2 name-label ( RW): xenserver200-201 name-description ( RW): master ( RO): 1e6bfa01-8785-49a9-b27e-351413463450 default-SR ( RW): 6e336c82-acc1-8858-1060-32cdc7016804 设置pool的自动启动1[root@node200 ~]# xe pool-param-set uuid=53e9df09-bb99-3014-ffdf-3a781a9a84d2 other-config:auto_poweron=true 设置虚拟机自动启动设置所有虚拟机随物理主机开机自动启动1[root@xenserver ~]# for i in `xe vm-list params=uuid --minimal|sed 's/,/ /g'`;do xe vm-param-set uuid=$i other-config:auto_poweron=true;done 如果只需要设置单台虚拟机随物理主机开机自动启动，则根据虚拟机的UUID来指定auto_poweron=true1[root@node200 ~]# xe vm-param-set uuid=107aa1fd-12ad-55b3-6a9e-3db624c45bf6 other-config:auto_poweron=true]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[判断虚拟机是否灵活]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-3%2F</url>
      <content type="text"><![CDATA[灵活的虚拟机必须满足下面三个条件： 虚拟机的所有虚拟磁盘必须置于共享存储中 虚拟机一定不能连接到配置的本地 DVD 驱动器 虚拟机的虚拟网络接口应位于池范围内的网络中 通过这三个条件统一来判断，只要有一项不满足，虚拟机就是不灵活的。另外可以直接使用xe diagnostic-vm-status uuid=[vm uuid] 命令判断虚拟机是否灵活。 灵活的虚拟机，最后显示VM is agile1234567891011121314151617181920212223242526272829303132333435[root@xenserver-8 ~]# xe diagnostic-vm-status uuid=7e2bdfa7-3b34-69c2-812e-6a36d3c080eauuid ( RO) : 7e2bdfa7-3b34-69c2-812e-6a36d3c080ea name-label ( RW): cy-windows power-state ( RO): running possible-hosts ( RO): 4ee4bd6c-57fd-45cb-b305-78caa61e4aca; 8f6ecd3d-8474-4208-9593-2e03e687249dChecking to see whether disks are attachableuuid ( RO) : cf507bfe-7369-15b0-3e3c-d4fe3727a7a3 vdi-uuid ( RO): &lt;not in database&gt; empty ( RO): true device ( RO): hdb userdevice ( RW): 1 mode ( RW): RO type ( RW): CD attachable ( RO): true storage-lock ( RO): falseuuid ( RO) : bd56ab95-5ac2-9a3c-c252-888bf06ca2e2 vdi-uuid ( RO): 8b79fae7-e488-4087-b787-b3a99fe282c5 empty ( RO): false device ( RO): hda userdevice ( RW): 0 mode ( RW): RW type ( RW): Disk attachable ( RO): true storage-lock ( RO): falseChecking to see whether VM can boot on each hostxenserver-6 : OK xenserver-8: OKVM is agile. 不灵活的虚拟机，最后显示VM is not agile，并提示不灵活原因是使用了本地存储。123456789101112131415161718192021222324252627282930313233343536[root@xenserver-8 ~]# xe diagnostic-vm-status uuid=e15312b4-6222-46e4-90fe-e0f13bb3b7b1uuid ( RO) : e15312b4-6222-46e4-90fe-e0f13bb3b7b1 name-label ( RW): test power-state ( RO): running possible-hosts ( RO): 4ee4bd6c-57fd-45cb-b305-78caa61e4aca; 8f6ecd3d-8474-4208-9593-2e03e687249dChecking to see whether disks are attachableuuid ( RO) : e73e640a-9d18-195a-9250-a359129d819c vdi-uuid ( RO): 055c23d2-405f-409c-a460-d5c1feced54b empty ( RO): false device ( RO): xvda userdevice ( RW): 0 mode ( RW): RW type ( RW): Disk attachable ( RO): true storage-lock ( RO): falseuuid ( RO) : 20c602f4-b70d-949d-9bda-016851923593 vdi-uuid ( RO): &lt;not in database&gt; empty ( RO): true device ( RO): xvdb userdevice ( RW): 1 mode ( RW): RO type ( RW): CD attachable ( RO): true storage-lock ( RO): falseChecking to see whether VM can boot on each hostxenserver-6 : Cannot start here [VM requires access to SR: cddf84c1-853d-8752-dbdf-ce8b17372b13 (Local storage)] xenserver-8: OKVM is not agile because: VM requires access to non-shared SR: cddf84c1-853d-8752-dbdf-ce8b17372b13 (Local storage). SR must both be marked as shared and a properly configured PBD must be plugged-in on every host]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[虚拟机进不了控制台或者挂住]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-2%2F</url>
      <content type="text"><![CDATA[在RHEL6、Centos6、Win2008R2、Win7以上操作系统的全虚VM，安装tools之后重启进不了控制台（半虚的VM正常），是因为VM参数platform:viridian设置为true，由于xapi会判断如果xs-tools是up-to-date，并且没有viridian，就会自动加上并设置true，需要手工修改platform:viridian的值为false。 解决方法查看VM的platform:viridian值为true1234567891011121314151617181920212223242526272829303132333435363738[root@xenserver-169 ~]# xe vm-param-list uuid=eae5ea6d-2b41-f390-626d-bf33c5f8ddcauuid ( RO) : eae5ea6d-2b41-f390-626d-bf33c5f8ddca name-label ( RW): wuyz1 name-description ( RW): Imported by XenServer Conversion Manager. user-version ( RW): 1 is-a-template ( RW): false is-a-snapshot ( RO): false snapshot-of ( RO): &lt;not in database&gt; snapshots ( RO): snapshot-time ( RO): 19700101T00:00:00Z snapshot-info ( RO): parent ( RO): &lt;not in database&gt; children ( RO): is-control-domain ( RO): false power-state ( RO): running memory-actual ( RO): 2147450880 memory-target ( RO): 0 memory-overhead ( RO): 19922944 memory-static-max ( RW): 2147483648 memory-dynamic-max ( RW): 2147483648 memory-dynamic-min ( RW): 2147483648 memory-static-min ( RW): 16777216 suspend-VDI-uuid ( RW): &lt;not in database&gt; suspend-SR-uuid ( RW): &lt;not in database&gt; VCPUs-params (MRW): VCPUs-max ( RW): 1 VCPUs-at-startup ( RW): 1 actions-after-shutdown ( RW): Destroy actions-after-reboot ( RW): Restart actions-after-crash ( RW): Restart console-uuids (SRO): d5ec329c-d6ab-52be-ddb9-59d6c8496f84 platform (MRW): viridian: true; timeoffset: 0; apic: true; pae: true; acpi: true; stdvga: 0; nx: true allowed-operations (SRO): changing_dynamic_range; migrate_send; pool_migrate; changing_VCPUs_live; suspend; hard_reboot; hard_shutdown; clean_reboot; clean_shutdown; pause; checkpoint; snapshot current-operations (SRO): blocked-operations (MRW): allowed-VBD-devices (SRO): 1; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15 allowed-VIF-devices (SRO): 1; 2; 3; 4; 5; 6 possible-hosts ( RO): 87e2c0c0-dff2-4770-839b-40b4ff90de32 将VM参数platform:viridian设置为false，重启VM。12[root@xenserver-169 ~]# xe vm-param-set platform:viridian=false uuid=eae5ea6d-2b41-f390-626d-bf33c5f8ddca[root@xenserver-169 ~]# xe vm-reboot uuid=eae5ea6d-2b41-f390-626d-bf33c5f8ddca force=true]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux虚拟机安装、卸载XenServer Tools]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vm-1%2F</url>
      <content type="text"><![CDATA[XenServer下Linux虚拟机安装、卸载XenServer Tools。 安装先选择要安装Tools的虚拟机，在XenCenter上挂载xs-tools.iso进入要安装的虚拟机Console，先后执行下面命令： 123# mkdir /mnt/xs-tools# mount /dev/xvdd /mnt/xs-tools# /mnt/xs-tools/Linux/install.sh 如果是自定义模板安装的虚拟机，则如下：123# mkdir /mnt/xs-tools# mount /dev/cdrom /mnt/xs-tools# /mnt/xs-tools/Linux/install.sh reboot重启虚拟机，生效OK。 卸载12345[root@hvm ~]# rpm -qa | grep xe-guestxe-guest-utilities-6.2.0-1120.x86_64xe-guest-utilities-xenstore-6.2.0-1120.x86_64[root@hvm ~]# rpm -e xe-guest-utilities-6.2.0-1120.x86_64[root@hvm ~]# rpm -e xe-guest-utilities-xenstore-6.2.0-1120.x86_64]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[虚拟机挂载磁盘个数]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vdi-2%2F</url>
      <content type="text"><![CDATA[原来的计算方式虚拟机可以挂载磁盘设备为allowed-VBD-devices中的值，值的个数为还可以挂载的VDI个数（在XenCenter上虚拟机挂载VDI时也是通过这种方式来计算校验） 虚拟机挂载磁盘个数默认最大值为16（除去CD光驱就是15个） 如果虚拟机系统异常，获取不到操作系统、未安装tools等，则挂载磁盘个数最大值为4（除去CD光驱就是3个） 查看allowed-VBD-devices的值，虚拟机还可以挂载的VDI为4个，可用的vbd的userdevice分别为1,3,4,15123456789101112[root@xenserver-155 ~]# xe vm-param-list uuid=85db576d-d694-af0a-6430-267532168458uuid ( RO) : 85db576d-d694-af0a-6430-267532168458 name-label ( RW): lh-src name-description ( RW): user-version ( RW): 1 is-a-template ( RW): false is-a-snapshot ( RO): false snapshot-of ( RO): &lt;not in database&gt; snapshots ( RO): snapshot-time ( RO): 19700101T00:00:00Z allowed-VBD-devices (SRO): 1,3,4,15 现在的挂载不以allowed-VBD-devices来创建，查找VM的VBD的userdevice，设置一个未被使用的值就可以了。这样就没有数量的限制。 使用CLI方式设置userdevice（对应命令行中的device）为7，给虚拟机与VDI创建vbd12[root@node200 ~]# xe vbd-create bootable=false device=7 mode=RW type=Disk unpluggable=true vdi-uuid=456a16a1-51e3-4cee-9eb4-a3b246078cc0 vm-uuid=107aa1fd-12ad-55b3-6a9e-3db624c45bf68a656533-c3f8-3ba2-6aa8-278eab6ac338 C#代码挂载VDI给虚拟机，只要userdevice未被虚拟机上的其他VBD使用，就可以挂载。12345678910VBD vbd = new VBD ();vbd.bootable = false;vbd.userdevice = &quot;6&quot;; //查找VM的VBD的userdevice，设置一个未被使用的值vbd.mode = vbd_mode.RW;vbd.type = vbd_type.Disk;vbd.unpluggable = true;vbd.VDI = VDI.get_by_uuid(session, &quot;8cc20e68-62eb-40bd-9af4-492f4e8c2ca9&quot; );vbd.VM = VM.get_by_uuid(session, &quot;dc600733-932d-0e77-e9bf-9deb2e4e5813&quot; );VBD.create(session, vbd);]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[虚拟机VDI扩容]]></title>
      <url>%2F2017%2F03%2F16%2Fxenserver-vdi-1%2F</url>
      <content type="text"><![CDATA[虚拟机的磁盘容量不够时，可以进行扩容操作。两种方式： 增加新的虚拟磁盘,比较简单，直接添加数据盘通过vdi-create、vbd-plug就可以。 扩展磁盘的大小，增大磁盘的容量。 下面以Centos7为例，扩展系统盘的大小从10G扩展到50G。 在XenServer主机上扩容虚拟磁盘扩展vdi的大小为50G1[root@xenserver155 ~]# xe vdi-resize uuid=13575f55-e55d-7f49-f22d-62427cb8fa22 disk-size=50GiB disk-size的单位可以为GiB、MiB，不带单位默认为B 在虚拟机上扩展磁盘分区增加磁盘分区增加一个大小为40G的新分区xvda31[root@controller161 ~]# fdisk /dev/xvda 使分区生效1[root@controller161 ~]# partprobe 或者1[root@controller161 ~]# reboot 创建pv创建pv12[root@controller161 ~]# pvcreate /dev/xvda3Physical volume "/dev/xvda3" successfully created 查看pv，xvda3的pv已经创建12345678910111213141516171819202122[root@controller161 ~]# pvdisplay --- Physical volume --- PV Name /dev/xvda2 VG Name centos PV Size 9.51 GiB / not usable 2.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 2434 Free PE 10 Allocated PE 2424 PV UUID kkBtLT-uY9U-7ChE-DikQ-PiPT-DNYL-oCvKID --- Physical volume --- PV Name /dev/xvda3 VG Name PV Size 40.00 GiB / not usable 4.00 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 10239 Free PE 0 Allocated PE 10239 PV UUID h24gB9-Toe2-Pq2J-9dse-S0nK-pJUC-oz5bdc 扩容vg查看扩容之前的vg123456789101112131415161718192021[root@controller161 ~]# vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 6 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 9.50 GiB PE Size 4.00 MiB Total PE 12673 Alloc PE / Size 12663 / 9.46 GiB Free PE / Size 10 / 40.00 MiB VG UUID A9ge4D-F5u0-8zAS-Jspw-fWou-Q89d-EVOH0I 扩容pv到vg12[root@controller161 ~]# vgextend centos /dev/xvda3Volume group "centos" successfully extended 查看扩容之后的vg123456789101112131415161718192021[root@controller161 ~]# vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 6 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 49.50 GiB PE Size 4.00 MiB Total PE 12673 Alloc PE / Size 12663 / 49.46 GiB Free PE / Size 10 / 40.00 MiB VG UUID A9ge4D-F5u0-8zAS-Jspw-fWou-Q89d-EVOH0I 扩容lv查看扩容前的lv12345678910111213141516171819202122232425262728293031323334[root@controller161 ~]# lvdisplay --- Logical volume --- LV Path /dev/centos/swap LV Name swap VG Name centos LV UUID o2UgDI-TFjU-WGIP-pZQq-XAD3-5SdQ-WbSYSR LV Write Access read/write LV Creation host, time localhost, 2016-06-28 10:35:23 +0800 LV Status available # open 2 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:1 --- Logical volume --- LV Path /dev/centos/root LV Name root VG Name centos LV UUID BlUZvD-D8Pf-OC0b-Jrje-CT1g-u0QC-FJf3Eo LV Write Access read/write LV Creation host, time localhost, 2016-06-28 10:35:24 +0800 LV Status available # open 1 LV Size 8.46 GiB Current LE 12407 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 扩容lv1[root@controller161 ~]# lvextend /dev/centos/root /dev/xvda3 查看扩容之后的lv12345678910111213141516171819202122232425262728293031323334[root@controller161 ~]# lvdisplay --- Logical volume --- LV Path /dev/centos/swap LV Name swap VG Name centos LV UUID o2UgDI-TFjU-WGIP-pZQq-XAD3-5SdQ-WbSYSR LV Write Access read/write LV Creation host, time localhost, 2016-06-28 10:35:23 +0800 LV Status available # open 2 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:1 --- Logical volume --- LV Path /dev/centos/root LV Name root VG Name centos LV UUID BlUZvD-D8Pf-OC0b-Jrje-CT1g-u0QC-FJf3Eo LV Write Access read/write LV Creation host, time localhost, 2016-06-28 10:35:24 +0800 LV Status available # open 1 LV Size 48.46 GiB Current LE 12407 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 查看分区的文件系统类型123456789101112131415[root@controller161 ~]# parted /dev/xvdaGNU Parted 3.1Using /dev/xvdaWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) pModel: Xen Virtual Block Device (xvd)Disk /dev/xvda: 53.7GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags:Number Start End Size Type File system Flags1 1049kB 525MB 524MB primary xfs boot2 525MB 10.7GB 10.2GB primary lvm3 10.7GB 53.7GB 42.9GB primary lvm 扩展文件系统1[root@controller161 ~]# xfs_growfs /dev/mapper/centos-root centos6及以下的文件系统为ext3、ext4 ，执行resize2fs /dev/mapper/centos-root centos7默认文件系统为xfs，执行 xfs_growfs /dev/mapper/centos-root 查看容量变化原容量10G，扩容之后的大小为50G12345678910[root@controller161 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 49G 8.3G 41G 18% /devtmpfs 1.9G 0 1.9G 0% /devtmpfs 1.8G 0 1.8G 0% /dev/shmtmpfs 1.8G 8.5M 1.8G 1% /runtmpfs 1.8G 0 1.8G 0% /sys/fs/cgroup/dev/xvda1 497M 169M 329M 34% /boottmpfs 354M 0 354M 0% /run/user/0tmpfs 354M 0 354M 0% /run/user/990]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[查看SR与VDI路径]]></title>
      <url>%2F2017%2F03%2F15%2Fxenserver-sr-4%2F</url>
      <content type="text"><![CDATA[查看SR与VDI在物理主机上的文件路径。SR类型： NFS与ISO LVM与HBA NFS与ISO查看NFS与ISO类型的SR，使用 df -h 命令，SR目录文件放在/var/run/sr-mount/[sr-uuid]下，子目录放置的是VDI文件123456789[root@xenserver153 sr-mount]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda1 3.9G 1.9G 1.9G 51% /tmpfs 1.6G 64K 1.6G 1% /dev/shm192.168.217.129:/WYY-NFS/07bec9d7-aa21-ddea-0bbd-517559759cf3 50G 7.1G 40G 16% /var/run/sr-mount/07bec9d7-aa21-ddea-0bbd-517559759cf310.10.152.1:/os_iso 372G 314G 40G 89% /var/run/sr-mount/f11c03b3-13b5-93b0-b2df-0f8c156336a210.0.0.25:/nfs/6225306e-bdc1-f6ab-8dbc-1e8bcf42d39b 197G 185G 2.3G 99% /var/run/sr-mount/6225306e-bdc1-f6ab-8dbc-1e8bcf42d39b 12345[root@xenserver153 sr-mount]# ls /var/run/sr-mount/6225306e-bdc1-f6ab-8dbc-1e8bcf42d39b/387d5bcf-26ac-4316-9600-7bdeacd67e46.vhd536fc728-05e7-4afa-80d6-cde8401dd5c2.vhdf168399e-86b7-47e3-b344-655446ccefd3.vhdfc457d9e-2ef4-4111-bcc4-6b3ab446e464.vhd LVM与HBA查看本地SR与HBA SR，使用 ls /dev/VG_XenStorage-[sr-uuid]下，子目录放置的是VDI文件1234567891011121314151617[root@xenserver-171 /]# ls /dev/VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0ebMGTVHD-17eb0cbb-e5e0-42d8-b566-c108021b95f6VHD-1ff5aa97-d289-4c5d-9168-e27261987098VHD-25296331-af1d-4539-b82b-059da6156978VHD-2b251f67-bc91-4b1d-a278-c07bc26bbd27VHD-4a6c24a5-f724-40a4-bf5b-0171086bf757VHD-636a3fff-3c90-41a8-b448-7e675858df54VHD-7d4fcfcc-96d6-4995-aa47-cd4c9ceae592VHD-860332e4-7d33-4052-bb38-feb0508ed7a5VHD-9164771a-cb81-4a77-a6b9-58f9c0180eafVHD-951ea29e-9407-4b9d-af5e-cdded140e707VHD-bdb7c50c-6167-4a9c-ae71-2cf63e229091VHD-d0e5ece1-d826-48a0-a273-866c084e0f68VHD-ebb9defa-49c3-4c60-b7ac-88efe1518f96VHD-ed9d01ad-ea2e-40f1-a21f-444cff043aedVHD-f9e07921-0a0a-438d-a87f-fb17249a07f5 LVM类型的本地SR，可以使用 lvs 命令来查看全部VDI1234567[root@xenserver-171 /]# lvs LV VG Attr LSize Origin Snap% Move Log Copy% Convert MGT VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0eb -wi-a- 4.00M VHD-0db56453-2039-4df6-bda9-04d387e74015 VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0eb -wi--- 24.05G VHD-16e5938c-7668-4715-bbce-109940da1af5 VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0eb -wi--- 24.05G VHD-17eb0cbb-e5e0-42d8-b566-c108021b95f6 VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0eb -ri-ao 5.78G VHD-1ff5aa97-d289-4c5d-9168-e27261987098 VG_XenStorage-1a659635-c40c-95b9-4958-5afd1df4e0eb -wi-ao 10.03G]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[卸载PBD失败 Error code: SR_BACKEND_FAILURE_202 Error parameters]]></title>
      <url>%2F2017%2F03%2F15%2Fxenserver-sr-3%2F</url>
      <content type="text"><![CDATA[后端存储问题，卸载PBD失败 Error code: SR_BACKEND_FAILURE_202 Error parameters: , General backend error [opterr=Command os.stat(/var/run/sr-mount/f9facf7f-501e-04ad-a136-37d500bd3a3c) failed (5): failed], 操作步骤在主机上probe sr，显示后端存储有问题12345678910111213141516171819202122[root@xenserver-161 ~]# xe sr-probe type=lvmoiscsi device-config:target=192.168.212.220 device-config:targetIQN=iqn.1992-08.com.netapp:sn.1936860767 host-uuid=d1e73513-e1d9-4850-93c9-241c6975e2a4There was an SR backend failure.status: non-zero exitstdout:stderr: Traceback (most recent call last): File "/opt/xensource/sm/LVMoISCSISR", line 578, in ? SRCommand.run(LVHDoISCSISR, DRIVER_INFO) File "/opt/xensource/sm/SRCommand.py", line 336, in run sr = driver(cmd, cmd.sr_uuid) File "/opt/xensource/sm/SR.py", line 135, in __init__ self.load(sr_uuid) File "/opt/xensource/sm/LVMoISCSISR", line 168, in load self._LUNprint(sr_uuid) File "/opt/xensource/sm/LVMoISCSISR", line 405, in _LUNprint self.iscsi.print_LUNs() File "/opt/xensource/sm/ISCSISR.py", line 621, in print_LUNs obj._query(vdi_path, LUNid) File "/opt/xensource/sm/LUNperVDI.py", line 41, in _query self.uuid = scsiutil.gen_uuid_from_string(scsiutil.getuniqueserial(path)) File "/opt/xensource/sm/scsiutil.py", line 77, in gen_uuid_from_string raise util.CommandException(1)util.CommandException: 1 找到已挂载的PBD12345678910111213[root@xenserver-161 ~]# xe pbd-list sr-uuid=f9facf7f-501e-04ad-a136-37d500bd3a3cuuid ( RO) : 43ab5ef1-6153-7cc8-5b8e-f632dbab9546 host-uuid ( RO): d1e73513-e1d9-4850-93c9-241c6975e2a4 sr-uuid ( RO): f9facf7f-501e-04ad-a136-37d500bd3a3c device-config (MRO): serverpath: /NFS1; server: 192.168.213.52; chappassword: ; chapuser: currently-attached ( RO): trueuuid ( RO) : 8c05886d-aa7d-5208-7c5c-1a9e04648d0b host-uuid ( RO): 37fba4cd-26bc-4477-880c-0268375da64c sr-uuid ( RO): f9facf7f-501e-04ad-a136-37d500bd3a3c device-config (MRO): serverpath: /NFS1; server: 192.168.213.52; chappassword: ; chapuser: currently-attached ( RO): false 卸载PBD123[root@xenserver-161 ~]# xe pbd-unplug uuid=43ab5ef1-6153-7cc8-5b8e-f632dbab9546Error code: SR_BACKEND_FAILURE_202Error parameters: , General backend error [opterr=Command os.stat(/var/run/sr-mount/f9facf7f-501e-04ad-a136-37d500bd3a3c) failed (5): failed], 解决方法使用umount命令卸载NFS存储池1[root@xenserver-161 ~]# umount -f /var/run/sr-mount/f9facf7f-501e-04ad-a136-37d500bd3a3c 执行pbd-unplug、pbd-destroy、sr-forget123[root@xenserver-161 ~]# xe pbd-unplug uuid=43ab5ef1-6153-7cc8-5b8e-f632dbab9546[root@xenserver-161 ~]# xe pbd-destroy uuid=43ab5ef1-6153-7cc8-5b8e-f632dbab9546[root@xenserver-161 ~]# xe sr-forget uuid=f9facf7f-501e-04ad-a136-37d500bd3a3c]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer存储池操作]]></title>
      <url>%2F2017%2F03%2F14%2Fxenserver-sr-2%2F</url>
      <content type="text"><![CDATA[存储池有4种状态：活动、未挂载、挂载异常、异常。 活动：存储池正常挂载在物理主机上面。共享存储池同时挂载给资源池里所有的物理主机。活动状态下可以执行卸载操作。 挂载异常：存储池与物理主机有PBD连接信息，但是未挂载到物理主机上，或者未同时挂载给资源池里所有物理主机。挂载异常状态下可以执行挂载、移除、删除操作。 未挂载：存储池与物理主机没有PBD连接信息。活动状态下的存储池进行了分离操作后出现该状态。未挂载状态下可以执行重新连接、移除、删除操作。 异常：存储池与主机有连接信息，并且挂载到主机上，但是存储池本身出现异常（后端存储LUN不存在、网络异常等）。可以通过xe sr-scan超时来判断。 存储池操作挂载存储池当前状态为“挂载异常”，即PBD的currently-attached=false，可以进行挂载操作，将PBD连接挂载到物理主机上。 如果没有PBD，先创建再挂载1xe pbd-create host-uuid=[HOST_UUID] sr-uuid=[SR_UUID] 挂载PBD1xe pbd-plug uuid=[PBD_UUID] 卸载存储池当前状态为“活动”，即PBD的currently-attached=true，可以进行卸载载操作，将PBD连接从物理主机上卸载。1xe pbd-unplug uuid=[PBD_UUID] 删除存储池当前状态为“挂载异常”或者“未挂载”，可以进行删除操作。存储池数据也会删除。 1xe sr-destroy uuid=[SR_UUID] 移除将存储池与物理主机的联系完全删除，在物理主机上无法看到该存储池。移除的存储池里面的数据会保留，但是虚拟磁盘与虚拟机的关联信息将被删除，磁盘数据还在，再次创建时可以重新连接或者格式化。 存储池当前状态为“挂载异常”、“未挂载”或者“异常”，可以进行移除操作。有挂载了PBD的，先卸载PBD。 有挂载了PBD的，先卸载PBD1xe pbd-unplug uuid=[PBD_UUID] 移除存储池1xe sr-forget uuid=[SR_UUID] 分离操作删除存储池与物理主机的PBD连接信息，使存储池无法访问。需要恢复，使用”重新连接”重新输入连接信息，才可以访问。存储池当前状态为“活动”、“挂载异常”，可以进行分离操作。存储池里面的虚拟磁盘如果有挂载给正在使用的虚拟机，则不可以进行分离操作。 卸载PBD1xe pbd-unplug uuid=[PBD_UUID] 删除PBD1xe pbd-destroy uuid=[PBD_UUID] 重新连接根据重新输入的路径信息，将存储池重新与物理主机建立起关联，使物理主机正常的访问存储池。存储池当前状态为“未挂载”，才可以进行重新连接。需要重新输入存储池的相应信息。例如：NFS类型需要重新输入存储路径、HBA类型需要重新输入目标LUN。 如果有PBD，先删除PBD1xe pbd-destroy uuid=[PBD_UUID] 创建PBD1xe pbd-create host-uuid=[HOST_UUID] sr-uuid=[SR_UUID] device-config:[key]=[value] 挂载PBD1xe pbd-plug uuid=[PBD_UUID] 格式化当移除iSCSI、HBA类型存储池之后，创建新存储池输入的路径与移除掉的存储池路径一致时，可以格式化原来的存储池数据。 重新引入存储池1xe sr-introduce shared=true type=[lvmiscsi/lvmohba] uuid=[SR_UUID] 创建PBD1xe pbd-create host-uuid=[HOST_UUID] sr-uuid=[SR_UUID] device-config:[key]=[value] 挂载PBD1xe pbd-plug uuid=[PBD_UUID] 挂载PBD之后，才能检测是否有VDI1xe vdi-list sr-uuid=[SR_UUID] 如果有VDI，删除VDI1xe vdi-destroy uuid=[VDI_UUID] 卸载PBD1xe pbd-unplug uuid=[PBD_UUID] 删除SR1xe sr-destroy uuid=[SR_UUID]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ISCSI操作总结]]></title>
      <url>%2F2017%2F03%2F14%2Fxenserver-sr-1%2F</url>
      <content type="text"><![CDATA[ISCSI操作总结。 增加ISCSI存储发现iscsi存储1iscsiadm -m discovery -t st -p &lt;ISCSI_IP&gt; 或者1iscsiadm -m discovery -t sendtargets -p &lt;ISCSI_IP&gt; 查看iscsi发现记录1iscsiadm -m node 登录iscsi存储1iscsiadm -m node -T &lt;TARGET_NAME&gt; -p &lt;ISCSI_IP&gt; -l 开机自动1iscsiadm -m node –T &lt;TARGET_NAME&gt; -p &lt;ISCSI_IP&gt; --op update -n node.startup -v automatic 删除iscsi存储登出iscsi存储1iscsiadm -m node -T &lt;TARGET_NAME&gt; -p &lt;ISCSI_IP&gt; -u 退出iscsi所有登录1iscsiadm -m node --logoutall=all 删除iscsi发现记录1iscsiadm -m node -o delete -T &lt;TARGET_NAME&gt; -p &lt;ISCSI_IP&gt; 登录需验证的iscsi开启认证1iscsiadm -m node -T &lt;TARGET_NAME&gt; -o update --name node.session.auth.authmethod --value=CHAP 添加用户1iscsiadm -m node -T &lt;TARGET_NAME&gt; --op update --name node.session.auth.username --value=[用户名] 添加密码1iscsiadm –m node –T &lt;TARGET_NAME&gt; –op update --name node.session.auth.password –-value=[密码] 查看登录的iscsi session1iscsiadm -m session 或者1iscsiadm --mode session 查看登录发现的所有iscsi1iscsiadm --mode node -l all 退出发现的所有iscsi1iscsiadm --mode node -u all]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于亲和与离散策略的XenServer资源调度负载均衡方案]]></title>
      <url>%2F2017%2F03%2F14%2Fxenserver-pool-2%2F</url>
      <content type="text"><![CDATA[弹性计算是根据业务需要在虚拟机中智能地分配合适的资源，保持业务的持续性和稳定性，从而实现资源池内各个物理主机的动态负载均衡，达到资源的充分利用。 弹性计算的实现采用贪心算法（贪婪算法），不从整体最优上加以考虑，而是某种意义上的局部最优解。 方案一设置一个阀值，只要资源池内的物理主机的cpu和mem不超过阀值，就不做调度。 亲和策略：可以把亲和的虚拟机当成一个整体VM组，考虑VM组整体的cpu和mem，放置在同一台主机或者同一个主机组。 离散策略：与亲和算法类似，不同的是放置虚拟机时，判断是否有离散（互斥）的VM，如果有的话，就不能放置在一起，那就选择下一台目标主机判断处理。 方案二设置一个阀值区间（即一个低阀值、一个高阀值），只要资源池内的物理主机cpu和mem在这个阀值区间内，就认为是负载均衡，不做调度。 算法一：局部贪心，局部贪心策略在每一次选择贪心选择时，只着眼于VM当前迁出或迁入能够得到的最优选择。 算法二、全局贪心算法，与局部贪心策略不同，全局贪心策略在选择一台迁出的VM时，也考虑了迁入这台物理主机对整个系统负载均衡情况的影响，根据当前负载情况，做出最优选择。 方案对比 方案一相对简单，相对负载均衡，可能从某种角度来说不是真正负载均衡。 方案二由于要考虑负载均衡，算法复杂，但可能出现来回动荡情况。 算法1：算法比2简单 算法2：整体考虑负载均衡，负载均衡的效果比1好，实现比较复杂 实现方案采用方案一。 规则调度弹性计算调度中的规则调度部分。 VM到VM规则 亲和：虚拟机应当放置在同一台主机或者同一个主机组 离散：虚拟机应当放置在不同的主机或者不同的主机组 VM到HOST规则 亲和：一组虚拟机放置到一组主机上 离散：一组虚拟机不能放置到一组主机上]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer组成异构资源池]]></title>
      <url>%2F2017%2F03%2F13%2Fxenserver-pool-1%2F</url>
      <content type="text"><![CDATA[由于现有硬件环境限制，或者后续硬件升级需要将不同CPU的物理主机组成XenServer资源池。默认情况下，异构的物理主机是不能组成资源池的，需要修改CPU的特征码，使用相同特征码的主机才可以强制组成资源池。 XenServer组成异构资源池两种情况： 只有两台主机的CPU不同：使用compare-cpu命令操作获取新的特征码 两台以上主机的CPU不同： 先两台主机compare-cpu命令操作获取新并设置的特征码，然后用新的特征码文件加一台主机执行同样的操作，直到获取最终的特征码； 对所有特征码使用“相与”计算 查看CPU信息两台主机，分别使用 xe host-cpu-info 命令查看cpu信息1234567891011121314[root@xenserver-179 ~]# xe host-cpu-infocpu_count : 16 socket_count: 2 vendor: AuthenticAMD speed: 2000.042 modelname: AMD Opteron(tm) Processor 6128 family: 16 model: 9 stepping: 1 flags: fpu de tsc msr pae mce cx8 apic mtrr mca cmov pat clflush mmx fxsr sse sse2 ht syscall nx constant_tsc nonstop_tsc pni popcnt hypervisor features: 00802009-178bfbff-000837ff-efd3fbff features_after_reboot: 00802009-178bfbff-000837ff-efd3fbff physical_features: 00802009-178bfbff-000837ff-efd3fbff maskable: full 1234567891011121314[root@xenserver-167 ~]# xe host-cpu-infocpu_count : 8 socket_count: 2 vendor: GenuineIntel speed: 2500.072 modelname: Intel(R) Xeon(R) CPU E5-2609 v2 @ 2.50GHz family: 6 model: 62 stepping: 4 flags: fpu de tsc msr pae mce cx8 apic sep mtrr mca cmov pat clflush mmx fxsr sse sse2 ht nx constant_tsc nonstop_tsc aperfmperf pni popcnt hypervisor arat features: 77bee3ff-bfebfbff-00000001-2c100800 features_after_reboot: 77bee3ff-bfebfbff-00000001-2c100800 physical_features: 77bee3ff-bfebfbff-00000001-2c100800 maskable: full 参数说明： features 当前 CPU 使用的 feature features_after_reboot 修改后重启使用的 feature physical_features CPU 物理上支持的 feature maskable：Full 表示该 CPU 硬件支持修改 FeatureMask 计算通过两台不同的CPU的features值，两者“相与”可以计算出COMMON MASK。 把前面xe host-cpu-info命令获取输出的所有cpu信息，在一台XenServer主机上分别保存成两个文件AMD6128.txt和E5-2609.txt 将compare-cpu脚本上传到主机上，进入compare-cpu所在的目录，赋权：chmod -R 777 compare-cpu12[root@xenserver-179 ~]# cd /usr/share/[root@xenserver-179 share]# chmod -R 777 compare-cpu 在compare-cpu所在的目录下输入：./compare-cpu AMD6128.txt E5-2609.txt -v123456789101112131415161718192021222324[root@xenserver-179 share]# ./compare-cpu AMD6128.txt E5-2609.txt -v file1:AMD6128.txt file2:E5-2609.txt pool_mask: ffffff7f-ffffffff-ffffffff-ffffffff CPU 1: model name: AMD Opteron(tm) Processor 6128 features: 00802009-178bfbff-000837ff-efd3fbff masking level: full CPU 2: model name: Intel(R) Xeon(R) CPU E5-2609 v2 @ 2.50GHz features: 77bee3ff-bfebfbff-00000001-2c100800 masking level: full Result: CPU 1 and CPU 2 are compatible for masking Mask type: 3 - CPU 1 and CPU 2 have a mutually exclusive set of features but support a common mask Mask: 00802009-178BFBFF-00000001-2C100800Traceback (most recent call last): File "./compare-cpu", line 279, in ? exit(0)TypeError: 'str' object is not callable 得出common mask为：00802009-178BFBFF-00000001-2C100800 修改features在每台XenServer上执行一下命令：1[root@xenserver-179 ~]# xe host-set-cpu-features features=00802009-178BFBFF-00000001-2C100800 uuid=【主机UUID】 然后重启每台主机。 强制组成资源池强制入池强制加入资源池，在主机上执行命令：1[root@xenserver-119 ~]# xe pool-join master-address=192.168.212.153 master-username=root master-password=xen123 force=true 如果强制入池，输出一下信息：You attempted an operation that was not allowed.reason: Network backends differ需要修改网络堆栈。 查看主机网络堆栈查看network.conf配置12[root@xenserver-119 ~]# cat /etc/xensource/network.confbridge 或者查看host参数12[root@xenserver-119 ~]# xe host-list params=software-versionsoftware-version (MRO) : product_version: 2.0.0; platform_name: xenserver; platform_version: 2.0.0; product_brand: xenserver; build_number: B00; hostname: localhost.localdomain; date: 2014-11-01; xapi: 1.3; xen: 4.1.6.1; linux: 2.6.32.43-0.4.1.citrix.2.0.0.xen; xencenter_min: 2.0; xencenter_max: 2.0; network_backend: bridge; xenserver:main: Base Pack, version 2.0.0, build B000 查看资源池内主机网络堆栈12[root@xenserver share]# cat /etc/xensource/network.confopenvswitch 1234[root@xenserver share]# xe host-list params=software-versionsoftware-version (MRO) : product_version: 2.0.0; platform_name: xenserver; platform_version: 2.0.0; product_brand: xenserver; build_number: B00; hostname: localhost.localdomain; date: 2014-11-01; xapi: 1.3; xen: 4.1.6.1; linux: 2.6.32.43-0.4.1.citrix.2.0.0.xen; xencenter_min: 2.0; xencenter_max: 2.0; network_backend: openvswitch; xenserver:main: Base Pack, version 2.0.0, build B000software-version (MRO) : product_version: 2.0.0; platform_name: xenserver; platform_version: 2.0.0; product_brand: xenserver; build_number: B00; hostname: localhost.localdomain; date: 2014-11-01; xapi: 1.3; xen: 4.1.6.1; linux: 2.6.32.43-0.4.1.citrix.2.0.0.xen; xencenter_min: 2.0; xencenter_max: 2.0; network_backend: openvswitch; xenserver:main: Base Pack, version 2.0.0, build B000 将当前主机修改成与资源池其他主机相同12345[root@xenserver-119 ~]# xe-switch-network-backend openvswitchCleaning up old ifcfg filesEnabling openvswitch daemonConfigure system for openvswitch networkingYou *MUST* now reboot your system 重启主机1[root@xenserver-119 ~]# reboot compare-cpu脚本获取common mask的compare-cpu脚本（Python代码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279#! /usr/bin/python## compare-cpu## Determines if the specified xe host-cpu-info results # are compatible for XenServer CPU masking## Copyright (c) 2010 Citrix Systems, Inc.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU Lesser General Public License as published# by the Free Software Foundation; version 2.1 only. with the special# exception on linking described in file LICENSE.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the# GNU Lesser General Public License for more details.#def findValue(filename,value): file_handle = open(filename, 'r') file_line = file_handle.readline() while file_line != "": start = file_line.find(value) if start != -1: # Remove any EOL chars if file_line.find("\n") != -1: file_line = file_line[0:file_line.find("\n")].rstrip() # +2 skips the colon and space return file_line[start+len(value)+2:len(file_line)] file_line = file_handle.readline() return ""def printFmt(output1, output2): pad_max=22 print output1.rjust(pad_max), output2def compareCPUs(a_features, a_mask_level, b_features, b_mask_level, pool_mask): mask = "" compare_results = ["CPU 1 and CPU 2 have identical feature sets, no masking required", "CPU 1 can be masked to CPU 2", "CPU 1 cannot be masked to CPU 2"] # verify if a mask is required at all if a_features == b_features: return 0, compare_results[0] # remove separating dashes a_features2 = a_features.replace("-","") b_features2 = b_features.replace("-","") pool_mask2 = pool_mask.replace("-","") for i in range(0, 32, 16): # Convert desired portion of the features string to an int a_int = int(a_features2[i:i+16],16) b_int = int(b_features2[i:i+16],16) p_int = int(pool_mask2[i:i+16],16) # Apply pool mask # Don't overwrite original a and b feature values - they are needed later because # pool mask only applies in the evaluation of the mask on join and therefore it # should not be included in the actual joint mask to be returned a_p_int = a_int &amp; p_int b_p_int = b_int &amp; p_int # Evaluate the first half if i==0: # If masking needed if a_p_int != b_p_int: # But not supported return not compatible if a_mask_level == "no": return 2, compare_results[2] # Masking needed, supported; verify if a can be masked to b elif a_p_int &amp; b_p_int != b_p_int: # No common base mask return 2, compare_results[2] # Store the first half of the required mask with formatting mask = featureIntToString(a_int &amp; b_int) mask += "-" # Evaluate the second half if i==16: # If masking needed if a_p_int != b_p_int: # But not supported return not compatible if a_mask_level != "full": return 2, compare_results[2] # Masking needed, supported; verify a can be masked to b elif a_p_int &amp; b_p_int == b_p_int: # Both halves can be masked OK mask += featureIntToString(a_int &amp; b_int) return 1, compare_results[1], mask else: # No common extended mask return 2, compare_results[2] else: # Only first half must be masked, mask OK mask += featureIntToString(a_int &amp; b_int) return 1, compare_results[1], maskdef findJointMask(a_features, a_mask_level, b_features, b_mask_level, pool_mask): joint_mask = "" joint_mask_results = ["CPU 1 does not support masking", "CPU 2 does not support masking", "CPU 1 does not support extended masking", "CPU 2 does not support extended masking", "CPU 1 and CPU 2 support a common mask"] # remove separating dashes a_features2 = a_features.replace("-","") b_features2 = b_features.replace("-","") pool_mask2 = pool_mask.replace("-","") for i in range(0, 32, 16): # Convert desired portion of the features string to an int a_int = int(a_features2[i:i+16],16) b_int = int(b_features2[i:i+16],16) p_int = int(pool_mask2[i:i+16],16) # Apply pool mask # Don't overwrite original a and b feature values - they are needed later because # pool mask only applies in the evaluation of the mask on join and therefore it # should not be included in the actual joint mask to be returned a_p_int = a_int &amp; p_int b_p_int = b_int &amp; p_int joint_mask_half_int = (a_p_int &amp; b_p_int) # Evaluate the first half if i==0: # if a needs to be masked, verify it can be if joint_mask_half_int != a_p_int and a_mask_level != "base" and a_mask_level != "full": return 0, joint_mask_results[0] # if b needs to be masked, verify it can be if joint_mask_half_int != b_p_int and b_mask_level != "base" and b_mask_level != "full": return 1, joint_mask_results[1] # Evaluate the second half if i==16: # if a needs to be masked, verify it can be if joint_mask_half_int != a_p_int and a_mask_level != "full": return 2, joint_mask_results[2] # if b needs to be masked, verify it can be if joint_mask_half_int != b_p_int and b_mask_level != "full": return 3, joint_mask_results[3] # Store the half with formatting joint_mask += featureIntToString(a_int &amp; b_int) if i==0: joint_mask += "-" return 4, joint_mask_results[4], joint_maskdef featureIntToString(features): # Removes the 0x, removes the L, pads with 0s to 16 bytes, and adds hyphens featureString = hex(features).lstrip("0x").rstrip("L").rjust(16,"0") featureString = featureString[0:8] + "-" + featureString[8:16] return featureStringimport sysfrom optparse import OptionParser,OptionGroup# Parse cmd line argumentsparser=OptionParser(usage="usage: %prog -v -p pool_mask file1 file2 | -c features1 mask_level1 features2 mask_level2")more_info=OptionGroup(parser, "More information", "The values specified for the file1 and file2 arguments must contain the output from the xe host-cpu-info command on the joining host and existing pool master respectively. If using the -c option, specify the physical_features and maskable values from xe host-cpu-info on the joining host and existing pool master as the features1/mask_level1 and features2/mask_level2 arguments respectively.")parser.add_option_group(more_info) parser.add_option("-p", "--pool_mask", help="An optional 32 byte hex value for use as a general CPU mask, equivalent to pool.other-config:cpuid_feature_mask", default="ffffff7f-ffffffff-ffffffff-ffffffff")parser.add_option("-v", "--verbose", help="Display verbose output", action="store_true", dest="verbose")parser.add_option("-c", "--cmdline", help="Use feature and mask values specified on the command line rather than from a file", action="store_true", dest="cmdline", default=False)(options, args) = parser.parse_args()# Validate argument counts and filenames if specifiedif options.cmdline: if len(args) !=4: parser.error("Incorrect number of arguments. The -c option requires 4 arguments.") exit(1)else: if len(args) != 2: parser.error("Incorrect number of arguments. 2 arguments required.") exit(1) for i in args: try: file_handle = open(i, 'r') except IOError: print print "File %s does not exist" % (i) print exit(1) else: file_handle.close# Parse/retrieve argument valuesif options.cmdline: features = [args[0], args[2]] mask_levels = [args[1], args[3]]else: features = [findValue(args[0], "physical_features"), findValue(args[1], "physical_features")] mask_levels = [findValue(args[0], "maskable"), findValue(args[1], "maskable")] modelnames = [findValue(args[0], "modelname"), findValue(args[1], "modelname")]# Validate and display valuesif options.verbose and not options.cmdline: print printFmt("file1:", args[0]) printFmt("file2:", args[1])if options.verbose: if options.cmdline: print printFmt("pool_mask:", options.pool_mask) if options.verbose: for i in [0,1]: print printFmt("CPU %s:" % (i+1), "") if not options.cmdline: printFmt("model name:", modelnames[i]) printFmt("features:", features[i]) printFmt("masking level:", mask_levels[i])# Evaluate masking compatibilityresults = compareCPUs(features[0], mask_levels[0], features[1], mask_levels[1], options.pool_mask)printif results[0] == 0: # Identical feature sets, no masking required printFmt("Result:", results[1]) printFmt("Mask type:", "n/a") printFmt("Mask:", "n/a")elif results[0] == 1: # CPU 1 can be masked to CPU 2 (use case 1) printFmt("Result:", "CPU 1 and CPU 2 are compatible for masking") printFmt("Mask type:","1 - CPU 1 has a superset of features to CPU 2") printFmt("Mask:","%s" % results[2])else: results = findJointMask(features[0], mask_levels[0], features[1], mask_levels[1], options.pool_mask) # if joint mask is possible if results[0] == 4: # and joint mask is the same as joining features if results[2] == features[0]: # CPU 2 can be masked to CPU 1 (use case 2) printFmt("Result:", "CPU 1 and CPU 2 are compatible for masking") printFmt("Mask type:","2 - CPU 1 has a subset of features to CPU 2") printFmt("Mask:",results[2]) else: # joint mask required on all hosts (use case 3) printFmt("Result:", "CPU 1 and CPU 2 are compatible for masking") printFmt("Mask type:","3 - CPU 1 and CPU 2 have a mutually exclusive set of features but support a common mask") printFmt("Mask:",results[2]) else: # joint mask required but not supported printFmt("Result:", "CPU 1 and CPU 2 are not compatible for masking because %s" % results[1])printexit(0) Java代码获取featuresJava代码获取features的common mask1234567891011121314151617181920212223242526272829303132333435363738394041424344private String getNewFeatures(List&lt;String&gt; featureList) &#123; // 特征码的第一部分 String feature1 = null; // 特征码的第二部分 String feature2 = null; // 特征码的第三部分 String feature3 = null; // 特征码的第四部分 String feature4 = null; for (String feature : featureList) &#123; String[] currentFeatures = feature.split( "-"); if (feature1 == null) &#123; feature1 = currentFeatures[0]; feature2 = currentFeatures[1]; feature3 = currentFeatures[2]; feature4 = currentFeatures[3]; continue; &#125; // 将之前的结果与当前特征码进行"与"计算 feature1 = bitAnd(feature1, currentFeatures[0]); feature2 = bitAnd(feature2, currentFeatures[1]); feature3 = bitAnd(feature3, currentFeatures[2]); feature4 = bitAnd(feature4, currentFeatures[3]); &#125; String newFeature = feature1 + "-" + feature2 + "-" + feature3 + "-" + feature4; return newFeature; &#125; private String bitAnd(String feature1, String feature2) &#123; Long decimalBit1 = Long. valueOf(feature1, 16); Long decimalBit2 = Long. valueOf(feature2, 16); Long bit1AndBit2 = decimalBit1 &amp; decimalBit2; String result = Long. toHexString(bit1AndBit2); // 不足8位的在前面补充0 StringBuffer zeroSb = new StringBuffer(); for (int i = 0; i &lt; 8 - result.length(); i++) &#123; zeroSb.append( "0"); &#125; return zeroSb.toString() + result; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer切换master主机]]></title>
      <url>%2F2017%2F03%2F13%2Fxenserver-host-5%2F</url>
      <content type="text"><![CDATA[XenServer切换master主机。 禁用HA如果资源池启用了HA，先禁用HA，否则跳过此步骤。1[root@xenserver ~]# xe pool-ha-disable 显示主机列出资源池内的物理主机1234567891011121314[root@xenserver ~]# xe host-listuuid ( RO) : 29fb4d42-258f-4ce1-a81b-519479eb1f3e name-label ( RW): xenserver155 name-description ( RW): Default install of xenserveruuid ( RO) : ba2fcb36-6788-4cc4-90bc-dceb7d3dd2d0 name-label ( RW): xenserver-119 name-description ( RW): Default install of xenserveruuid ( RO) : cc6f3970-ddb3-4c87-8487-efc034ce3b6b name-label ( RW): xenserver153 name-description ( RW): Default install of xenserver 切换新的master使用 xe pool-designate-new-master host-uuid=[host-uuid]，切换新的master为xenserver155主机。1[root@xenserver ~]# xe pool-designate-new-master host-uuid=29fb4d42-258f-4ce1-a81b-519479eb1f3e]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer主机远程开关机]]></title>
      <url>%2F2017%2F03%2F13%2Fxenserver-host-4%2F</url>
      <content type="text"><![CDATA[XenServer远程开关机有以下几种方式： WoL(Wake-on-LAN): 网络唤醒 IPMI(Intelligent Platform Management Interface): 智能型平台管理接口 iLO(HP Integrated Light-Out): To use this option, iLO must be enabled on the host and be connected to the network. For more information, see HP’s iLO documentation. DRAC(Dell Remote Access Controller): To use this option, the Dell supplemental pack must be installed on the host server to get DRAC support. For more information, see Dell’s DRAC documentation. WoL与IPMI的方式使用较多。 WoLWake-on-LAN简称WOL或WoL，我们一般将其称为“网络唤醒”、“远端唤醒”技术。WOL是一种技术，同时也是该技术的规范标准。Wake-on-LAN功能需要有主板和网卡的支持，在主板BIOS中的网卡设置中必须有“Wake On LAN”设置（开启：On），并且相应网卡也得支持该功能。 主板BIOS设置WoL在主板BIOS中的网卡设置“Wake On LAN”（开启：On） 查看网卡是否支持ethtool命令查看网卡是否支持 Wake on Lan12345678910111213141516171819[root@localhost lhd]# ethtool eth0Settings for eth0:Supported ports: [ TP MII ]Supported link modes: 10baseT/Half 10baseT/Full100baseT/Half 100baseT/FullSupports auto-negotiation: YesAdvertised link modes: 10baseT/Half 10baseT/Full100baseT/Half 100baseT/FullAdvertised auto-negotiation: YesSpeed: 100Mb/sDuplex: FullPort: MIIPHYAD: 32Transceiver: internalAuto-negotiation: onSupports Wake-on: pumbgWake-on: dCurrent message level: 0x00000007 (7)Link detected: yes 可以看到，ethtool把网卡的信息全部列出，我们只关心其中的这两项:Supports Wake-on: pumbgWake-on: d如果 wake-on 一项值 值为 d,表示禁用wake on lan 值为 g,表示启用 wake on lan 开启网卡唤醒如果支持，但是没开启可以设置开启1[root@localhost lhd]# ethtool -s eth0 wol g 或者修改配置永久生效1[root@localhost lhd]# echo '/usr/sbin/ethtool -s eth0 wol g' &gt;&gt; /etc/rc.d/rc.local 配置XenServer主机设置启动模式：1xe host-set-power-on-mode host=[host-uuid] power-on-mode=wake-on-lan 启动XenServer主机资源池主节点上，启动其他XenServer主机1xe host-power-on host=[host-uuid] IPMIIPMI的核心是一个专用芯片/控制器(叫做服务器处理器或基板管理控制器(BMC))，其并不依赖于服务器的处理器、BIOS或操作系统来工作，是一个单独在系统内运行的无代理管理子系统，只要有BMC与IPMI固件其便可开始工作，而BMC通常是一个安装在服务器主板上的独立的板卡。 主板BIOS设置IPMI主板BIOS设置”IPMI Over LAN”为On 网络、用户设置两种方式来设置： BIOS设置 进入IPMI Parameters，设置服务器ip/子网掩码 进入LAN User Confuguration，设置用户名、密码 ipmitool工具 配置XenServer主机设置启动模式为IPMI：1xe host-set-power-on-mode host=[host-uuid] power-on-mode=IPMI power-on-config:power_on_ip=[ip] power-on-config:power_on_user=[user] power-on-config:power_on_password=[password] 如果需要加密密码，则指定power_on_password_secret 启动、关闭、重启XenServer主机资源池主节点上，远程启动、关闭、重启其他XenServer主机123ipmitool -l lan -H [ip] -U [user] -P [password] power off （关机）ipmitool -l lan -H [ip] -U [user] -P [password] power on （开机）ipmitool -l lan -H [ip] -U [user] -P [password] power reset（重启）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer iscsiadm: initiator reported error (19 - encountered non-retryable iSCSI login failure)]]></title>
      <url>%2F2017%2F03%2F13%2Fxenserver-host-3%2F</url>
      <content type="text"><![CDATA[XenServer主机登陆ISCSI服务器出错：1234[root@xenserver119 ~]# iscsiadm -m node -T iqn.2006-01.com.openfiler:tsn.56vg -p 192.168.212.104 --loginLogging in to [iface: default, target: iqn.2006-01.com.openfiler:tsn.56vg, portal: 192.168.212.104,3260]iscsiadm: Could not login to [iface: default, target: iqn.2006-01.com.openfiler:tsn.56vg, portal: 192.168.212.104,3260]:iscsiadm: initiator reported error (19 - encountered non-retryable iSCSI login failure) 是由于使用CHAP方式，设置了用户密码。 解决方法登录ISCSI时，需要开启CHAP认证，添加用户密码（用户名与密码使用配置的用户名与密码） 发现ISCSI1iscsiadm -m discovery -t st -p [ISCSI_IP] ISCSI_IP:ISCSI服务端IP 例如：123[root@xenserver119 ~]# iscsiadm -m discovery -t st -p 192.168.212.104192.168.212.104:3260,1 iqn.2006-01.com.openfiler:tsn.vg2192.168.212.104:3260,1 iqn.2006-01.com.openfiler:tsn.56vg 开启认证1iscsiadm -m node -T [Target_name] -o update --name node.session.auth.authmethod --value=CHAP 例如：1[root@xenserver119 ~]# iscsiadm -m node -T iqn.2006-01.com.openfiler:tsn.56vg -o update --name node.session.auth.authmethod --value=CHAP 添加用户1iscsiadm -m node -T [Target_name] --op update --name node.session.auth.username --value=[用户名] 例如：1[root@xenserver119 ~]# iscsiadm -m node -T iqn.2006-01.com.openfiler:tsn.56vg --op update --name node.session.auth.username --value=root 添加密码1iscsiadm –m node –T [Target_name] –op update –name node.session.auth.password –-value=[密码] 例如：1[root@xenserver119 ~]# iscsiadm –m node –T iqn.2006-01.com.openfiler:tsn.56vg –op update –name node.session.auth.password –-value=root 登陆ISCSI1iscsiadm -m node -T [Target_name] -p [ISCSI_IP] -l 列如：123[root@xenserver119 ~]# iscsiadm -m node -T iqn.2006-01.com.openfiler:tsn.56vg -p 192.168.212.104 -lLogging in to [iface: default, target: iqn.2006-01.com.openfiler:tsn.56vg, portal: 192.168.212.104,3260]Login to [iface: default, target: iqn.2006-01.com.openfiler:tsn.56vg, portal: 192.168.212.104,3260]: successful]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer主机上设置ISCSI IQN]]></title>
      <url>%2F2017%2F03%2F13%2Fxenserver-host-2%2F</url>
      <content type="text"><![CDATA[XenServer主机上设置ISCSI IQN。 查看IQN查看ISCSI IQN的值123[root@xenserver-155 ~]# cat /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2015-02.com.example:a50ca784InitiatorAlias=xenserver-155 设置IQN设置ISCSI IQN的值1[root@xenserver-155 ~]# xe host-param-set other-config:iscsi_iqn=iqn.2015-02.com.example:a50ca784 uuid=3e184f09-0aaf-4c7b-ac70-bb4b10ddcd23]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer主机执行xe命令出错：Lost connection to the server]]></title>
      <url>%2F2017%2F03%2F10%2Fxenserver-host-1%2F</url>
      <content type="text"><![CDATA[在三台主机组成的资源池中，没有了master节点，执行xe命令出错：Lost connection to the server 三台主机IP： 192.168.222.181 192.168.222.182 192.168.222.183 查看pool配置文件查看pool配置文件，183为主节点123456[root@macro-222-181 ~]# cat /etc/xensource/pool.confslave:192.168.222.183[root@macro-222-182 ~]# cat /etc/xensource/pool.confslave:192.168.222.183[root@macro-222-183 ~]# cat /etc/xensource/pool.confslave:192.168.222.183 解决方法在183上禁用HA1[root@macro-222-183 ~]# xe host-emergency-ha-disable --force 设置183为主节点12[root@macro-222-183 ~]# xe pool-emergency-transition-to-masterHost agent will restart and transition to master in 10.000 seconds... 通知其他从节点123[root@macro-222-183 ~]# xe pool-recover-slavesb8deaa20-cfe9-4a54-86c8-f18aecdc48d4bf2faa7f-6f50-46d9-8043-f24b41a69464 禁用HA1[root@macro-222-183 ~]# xe pool-ha-disable]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo博客绑定域名]]></title>
      <url>%2F2017%2F03%2F09%2Fhexo-domain%2F</url>
      <content type="text"><![CDATA[注册域名并绑定GitHub博客。 域名注册在腾讯云注册hl10502.com域名并认证 DNS解析添加CNAME类型的DNS记录: CNAME: @ hl10502.github.io. CNAME: www hl10502.github.io. 主机记录@： 主机记录www： 添加CNAME文件hexo源码source目录下增加CNAME文件，不带后缀。 文件内容为 www.hl10502.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer HA could not be enabled on the Pool because a liveset could not be formed: check storage and network heartbeat paths]]></title>
      <url>%2F2017%2F03%2F09%2Fxenserver-ha-4%2F</url>
      <content type="text"><![CDATA[XenServer资源池两台主机，启用HA失败： HA could not be enabled on the Pool because a liveset could not be formed: check storage and network heartbeat paths 两台XenServer主机组成一个资源池： 192.168.200.4 192.168.200.5 日志分析查看主机192.168.200.4的xha日志1234567891011121314151617181920212223242526272829[root@xenserver04 ~]# tail -f /var/log/xha.log......Dec 12 09:51:01 CST 2014 [debug] SM: SF domain is updated [sfdomain = (@0)].Dec 12 09:51:40 CST 2014 [debug] SM: heartbeat is readyDec 12 09:51:40 CST 2014 [debug] SM: statefile is readyDec 12 09:51:40 CST 2014 [debug] SM: other hosts are readyDec 12 09:51:40 CST 2014 [info] Start Criteria: pool state=INIT; excluded=FALSE; liveset=EMPTYDec 12 09:52:40 CST 2014 [notice] Start Criteria: Start timeout (0x5)Dec 12 09:52:40 CST 2014 [err] Start timeout (1901) Cannot start HA daemon, because Start Criteria is not met for the local host. - Abort.Dec 12 09:52:40 CST 2014 [debug] SM: SF domain is updated [sfdomain = (D0)].Dec 12 09:52:41 CST 2014 [notice] HA daemon started shutdown process.Dec 12 09:52:41 CST 2014 [info] SM: sm_initialize(-1).Dec 12 09:52:41 CST 2014 [info] SC: hostweight_initialize(-1).Dec 12 09:52:41 CST 2014 [info] SC: script_initialize(-1).Dec 12 09:52:41 CST 2014 [info] Xapimon: xapimon_initialize(-1).Dec 12 09:52:41 CST 2014 [info] HB: hb_initialize(-1).Dec 12 09:52:41 CST 2014 [info] WD: (watchdog_close) label=heartbeat stopping watchdog timer.Dec 12 09:52:41 CST 2014 [info] WD: (watchdog_close) label=heartbeat watchdog timer has been stopped successfully.Dec 12 09:52:41 CST 2014 [debug] WD: watchdog id file /var/run/xhad.wd.id is updated.Dec 12 09:52:41 CST 2014 [info] BM: bm_initialize(-1).Dec 12 09:52:41 CST 2014 [info] LM: lm_initialize(-1).Dec 12 09:52:41 CST 2014 [info] WD: (watchdog_close) label=statefile stopping watchdog timer.Dec 12 09:52:41 CST 2014 [info] WD: (watchdog_close) label=statefile watchdog timer has been stopped successfully.Dec 12 09:52:41 CST 2014 [debug] WD: watchdog id file /var/run/xhad.wd.id is deleted.Dec 12 09:52:41 CST 2014 [info] COM: com_initialize(-1).Dec 12 09:52:41 CST 2014 [notice] HA daemon completed shutdown process....... 查看主机192.168.200.5的xha日志1234567891011121314151617181920212223[root@xenserver05 ~]# tail -f /var/log/xha.log......Dec 12 09:51:06 CST 2014 [notice] HA daemon started - built at Jun 14 09:15:40 EDT 2013 - Dec 12 09:51:06 CST 2014 [info] CONF: my_index=1 num_host=2 t1=7 T1=60 t2=7 T2=60 Wh=60 Ws=75 Tboot=120 Tenable=120 tXapi=60 TXapi=120 RestartXapi=1 TRestartXapi=300 Tlicence=30Dec 12 09:51:06 CST 2014 [info] LOG: logmask = 100000Dec 12 09:51:06 CST 2014 [info] LOG: OFF:18(DUMPPACKET)Dec 12 09:51:06 CST 2014 [info] LOG: OFF:19(TRACE)Dec 12 09:51:06 CST 2014 [info] LOG: ON :20(FH_TRACE)Dec 12 09:51:06 CST 2014 [info] LOG: OFF:21(LM_TRACE)Dec 12 09:51:06 CST 2014 [info] LOG: OFF:22(SCRIPT)Dec 12 09:51:06 CST 2014 [info] LOG: OFF:23(SC_WARNING)Dec 12 09:51:06 CST 2014 [info] COM: com_initialize(0).Dec 12 09:51:06 CST 2014 [info] SF: phase 0 initialization...Dec 12 09:51:06 CST 2014 [info] LM: lm_initialize(0).Dec 12 09:51:06 CST 2014 [info] BM: bm_initialize(0).Dec 12 09:51:06 CST 2014 [info] HB: hb_initialize(0).Dec 12 09:51:06 CST 2014 [err] HB: cannot bind socket address (IP address = 192.168.200.5:694). (sys 98)Dec 12 09:51:06 CST 2014 [info] BM: bm_initialize(-1).Dec 12 09:51:06 CST 2014 [info] LM: lm_initialize(-1).Dec 12 09:51:06 CST 2014 [info] COM: com_initialize(-1)....... 解决方法HA启动的守护进程使用固定端口694。通过日志分析，发现主机192.168.200.5上的端口694被占用。 执行命令 netstat -naup 查看端口，rpc.statd服务占用了694端口。12345678910[root@xenserver05 ~]# netstat -naupActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name udp 0 0 0.0.0.0:111 0.0.0.0:* 17758/portmap udp 0 0 192.168.200.5:123 0.0.0.0:* 26166/ntpd udp 0 0 127.0.0.1:123 0.0.0.0:* 26166/ntpd udp 0 0 0.0.0.0:123 0.0.0.0:* 26166/ntpd udp 0 0 0.0.0.0:58882 0.0.0.0:* - udp 0 0 0.0.0.0:691 0.0.0.0:* 17899/rpc.statd udp 0 0 0.0.0.0:694 0.0.0.0:* 17899/rpc.statd kill进程1[root@xenserver05 ~]# kill -9 17899 启用ha1[root@xenserver05 ~]# xe pool-ha-enable heartbeat-sr-uuids=285aae43-6838-09af-ad3d-afc2d75d2d1a 启用成功，再次查看，xhad使用了694端口123456789[root@xenserver05 ~]# netstat -naupActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name udp 0 0 0.0.0.0:111 0.0.0.0:* 17758/portmap udp 0 0 192.168.200.5:123 0.0.0.0:* 26166/ntpd udp 0 0 127.0.0.1:123 0.0.0.0:* 26166/ntpd udp 0 0 0.0.0.0:123 0.0.0.0:* 26166/ntpd udp 0 0 0.0.0.0:58882 0.0.0.0:* - udp 0 0 192.168.200.5:694 0.0.0.0:* 26346/xhad rpc.statdrpc.statd是NFS服务，这个进程实现了网络状态监控(NSM)RPC协议,通知NFS客户端什么时候一个NFS服务器非正常重启动.这个进程被nfslock服务自动启动，不需要用户的配置。 12rpcinfo -p localhost #查看本机的rpc信息cat /etc/sysconfig/nfs #查看NFS配置信息 要想关掉RHEL/CentOS的rpc.statd服务，我们只需要执行下面的命令就能搞定1service nfslock stop 为了不让下次自动启动，那就执行下面的命令1chkconfig nfslock off 停止rpcbind服务的命令1service portmap stop 禁止下次开机自动启动命令1chkconfig portmap off]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer HA Internal error: Xapi_ha.Xha_error(4)]]></title>
      <url>%2F2017%2F03%2F09%2Fxenserver-ha-3%2F</url>
      <content type="text"><![CDATA[XenServer资源池启用HA出现内部错误 Internal error: Xapi_ha.Xha_error(4) 这是因为ha的心跳盘有问题：ha_start_daemon: the HA daemon stopped without forming a liveset (4) 日志分析查看日志出错信息1234567891011121314151617[root@xenserver119 ~]# tail -f /var/log/xensource.log...Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|helpers] /opt/xensource/xha/ha_start_daemon exited with code 4 [stdout = ''; stderr = 'Thu Apr 30 09:36:23 CST 2015 ha_start_daemon: the HA daemon stopped without forming a liveset (4) ']Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [ warn|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|xapi_ha] /opt/xensource/xha/ha_start_daemon returned MTC_EXIT_WATCHDOG_ERROR (Watchdog error)Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|backtrace] Raised at xapi_ha.ml:62.8-24 -&gt; xapi_ha.ml:1040.20-50 -&gt; xapi_host.ml:591.1-36 -&gt; message_forwarding.ml:233.25-44 -&gt; rbac.ml:229.16-23Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|backtrace] Raised at rbac.ml:238.10-15 -&gt; server_helpers.ml:79.11-41Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|dispatcher] Server_helpers.exec exception_handler: Got exception INTERNAL_ERROR: [ Xapi_ha.Xha_error(4) ]Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|dispatcher] Raised at string.ml:150.25-34 -&gt; stringext.ml:108.13-29Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|backtrace] Raised at string.ml:150.25-34 -&gt; stringext.ml:108.13-29Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|xapi] Raised at server_helpers.ml:94.14-15 -&gt; pervasiveext.ml:22.2-9Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|host.ha_join_liveset R:f424a3fb1e8f|xapi] Raised at pervasiveext.ml:26.22-25 -&gt; pervasiveext.ml:22.2-9Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|dispatch:host.ha_join_liveset D:646ea3d5749a|xapi] Raised at pervasiveext.ml:26.22-25 -&gt; pervasiveext.ml:22.2-9Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40339 UNIX /var/xapi/xapi|dispatch:host.ha_join_liveset D:646ea3d5749a|backtrace] Raised at pervasiveext.ml:26.22-25 -&gt; server_helpers.ml:140.10-106 -&gt; server.ml:12966.23-171 -&gt; server_helpers.ml:119.4-7Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40236|Async.pool.enable_ha R:697af26c5213|backtrace] Raised at sExprParser.ml:109.34-89 -&gt; parsing.ml:138.39-75 -&gt; parsing.ml:160.4-28Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [error|xenserver119|40236|Async.pool.enable_ha R:697af26c5213|xapi_ha] Caught exception while calling Host.ha_join_liveset: 'xenserver119' ('OpaqueRef:4231a807-42ad-0cd9-fc8f-09136c61f60d') INTERNAL_ERROR: [ Xapi_ha.Xha_error(4) ]Apr 30 09:36:23 xenserver119 /opt/xensource/bin/xapi: [debug|xenserver119|40236|Async.pool.enable_ha R:697af26c5213|backtrace] Raised at hashtbl.ml:93.19-28 -&gt; debug.ml:144.36-65... 解决方法找到名称为”Metadata for HA”、”Statefile for HA”的两个HA检测盘123456789101112131415161718[root@xenserver119 ~]# xe vdi-list name-label="Metadata for HA"uuid ( RO) : e40806a1-1958-4583-9f87-8e95ed9650ba name-label ( RW): Metadata for HA name-description ( RW): Used for master failover sr-uuid ( RO): 6e336c82-acc1-8858-1060-32cdc7016804 virtual-size ( RO): 268435456 sharable ( RO): true read-only ( RO): false[root@xenserver119 ~]# xe vdi-list name-label="Statefile for HA"uuid ( RO) : ce50b8fc-1286-4861-bd3d-2bca2404f22d name-label ( RW): Statefile for HA name-description ( RW): Used for storage heartbeating sr-uuid ( RO): 6e336c82-acc1-8858-1060-32cdc7016804 virtual-size ( RO): 4194304 sharable ( RO): true read-only ( RO): false 删除HA检测盘12[root@xenserver119 ~]# xe vdi-destroy uuid=e40806a1-1958-4583-9f87-8e95ed9650ba[root@xenserver119 ~]# xe vdi-destroy uuid=ce50b8fc-1286-4861-bd3d-2bca2404f22d 重新启用HA，同时指定作为信号检测的存储池1[root@xenserver119 ~]# xe pool-ha-enable heartbeat-sr-uuids=6e336c82-acc1-8858-1060-32cdc7016804]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer HA Internal error: Xapi_ha.Xha_error(15)]]></title>
      <url>%2F2017%2F03%2F09%2Fxenserver-ha-2%2F</url>
      <content type="text"><![CDATA[XenServer资源池启用HA出现内部错误 Internal error: Xapi_ha.Xha_error(15) 这是因为xha进程已经启动了，启用失败。 日志分析查看日志出错信息12345678910[root@node200 ~]# tail -f /var/log/xensource.log...Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|helpers] /opt/xensource/xha/ha_set_pool_state init exited with code 15 [stdout = ''; stderr = 'Wed Nov 6 14:24:12 CST 2013 Could not set the pool state to INIT because the HA daemon may be present (15) ']Nov 6 14:24:12 jtt-cloud-node1 xapi: [ warn|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|xapi_ha] /opt/xensource/xha/ha_set_pool_state init returned MTC_EXIT_DAEMON_IS_PRESENT (Daemon is (already) present)Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|backtrace] Raised at xapi_ha.ml:62.8-24 -&gt; xapi_ha.ml:1450.20-60Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|xapi_ha] Caught exception while enabling HA: INTERNAL_ERROR: [ Xapi_ha.Xha_error(15) ]Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|backtrace] Raised at xapi_ha.ml:1539.8-11 -&gt; threadext.ml:20.20-24 -&gt; threadext.ml:20.62-65 -&gt; rbac.ml:229.16-23Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|backtrace] Raised at rbac.ml:238.10-15 -&gt; server_helpers.ml:79.11-41Nov 6 14:24:12 jtt-cloud-node1 xapi: [debug|jtt-cloud-node1|1754404|Async.pool.enable_ha R:d2ea42a29c9d|dispatcher] Server_helpers.exec exception_handler: Got exception INTERNAL_ERROR: [ Xapi_ha.Xha_error(15) ]... 启用HA过程启用HA时，创建心跳盘，会初始化pool的状态，检测是否启动ha守护进程，如果已经存在则会报错： Could not set the pool state to INIT because the HA daemon may be present 不存在，就会启动xha进程，监控池内主机是否发生心跳信号（存储检测、网络检测） 解决方法查找xhad进程，进程存在会生成一个/etc/xensource/xhad.conf配置文件，记录HA的心跳检测、主机等配置信息123[root@node200 ~]# ps aux |grep xhadroot 8167 0.8 0.4 426616 17100 ? SLl 11:08 0:01 xhad /etc/xensource/xhad.confroot 12219 0.0 0.0 112652 976 pts/3 S+ 11:10 0:00 grep --color=auto xhad kill进程1[root@node200 ~]# kill -9 8167 重新启用HA，同时指定作为信号检测的存储池1[root@node200 ~]# xe pool-ha-enable heartbeat-sr-uuids=6e336c82-acc1-8858-1060-32cdc7016804]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenServer HA]]></title>
      <url>%2F2017%2F03%2F09%2Fxenserver-ha-1%2F</url>
      <content type="text"><![CDATA[HA（High Availability）：XenServer高可用是一套自动化功能，旨在针对导致XenServer主机停机或者无法访问的问题（例如因物理原因造成网络连接中断或者主机硬件发生故障）制定计划并进行安全地恢复。 在XenServer资源池配置受保护虚拟机、启用高可用之后，高可用守护进程发送心跳信息，监听池内主机是否出现网络或者存储故障。通过选举算法选择一个或者一组故障主机，故障主机通过底层的主机保护机制（hypervisor-level fencing）自动重启来保护数据安全。在故障主机上受保护的虚拟机自动在其他正常的主机上启动，达到保护虚拟机正常运行的目的。 HA启用条件 XenServer资源池建议资源池内至少包含3个XenServer主机，且每个主机都是使用静态IP地址 共享存储池一个大小至少为356MB或更大的iSCSI或光纤通道LUN作为检测信号的存储池，创建两个磁盘： 4 MB 检测信号卷：用于检测信号 256 MB 元数据卷：存储池主服务器元数据，以便在主服务器故障转移时使用 为最大程度提高可靠性，建议使用专用的iSCSI存储，且不得用于任何其他用途。 灵活的虚拟机受保护的虚拟机必须是灵活的。 虚拟机的虚拟磁盘必须置于共享存储中 虚拟机一定不能连接到配置的本地 DVD 驱动器 虚拟机的虚拟网络接口应位于池范围内的网络中 启用过程设置启用HA的参数，创建两个磁盘，一个4M的“Statefile for HA”，用来检测网络信号，一个256M的“MetaData for HA”用来存储元数据信息，初始化资源池的状态，检测是否启动HA守护进程，如果未启动则启动Xha进程，实时监控池内物理主机。 可用性检测可用性检测分为：心跳盘检测和存储盘检测。 心跳盘检测检测基于socket通信，不是ping。各个物理主机通过管理网络发送UDP数据包。UDP是在传输层，面向无连接的用户数据包协议，ICMP是Internet控制报文协议，面向连接的网络层，ping基于ICMP。心跳盘上记录的是物理主机之间通讯的信息。 存储盘检测存储池主服务器元数据，以便在主服务器故障转移时使用，所有主机可见的和可写的一个共享SR的VDI。存储盘上存放各物理主机的虚拟机列表，它包含着虚拟机的CPU预留及内存开销信息，master主机可以访问资源池内所有的虚拟机的信息。 启用HA查看虚拟机是否灵活的虚拟机，灵活的虚拟机才受高可用保护。1xe diagnostic-vm-status uuid=&lt;VM_UUID&gt; VM_UUID：虚拟机UUID 例如：UUID为0ec2a592-29ad-efe1-6ac7-94cafb50d37e的虚拟机为灵活的虚拟机123456789101112131415161718192021222324252627282930313233343536[root@node200 ~]# xe diagnostic-vm-status uuid=0ec2a592-29ad-efe1-6ac7-94cafb50d37euuid ( RO) : 0ec2a592-29ad-efe1-6ac7-94cafb50d37e name-label ( RW): CentOS 6 power-state ( RO): running possible-hosts ( RO): 1e6bfa01-8785-49a9-b27e-351413463450; 7a7998f3-64d9-45df-8538-589e3f32192fChecking to see whether disks are attachableuuid ( RO) : 8a50eb50-37a2-6c6d-7ac5-be4a9318b40c vdi-uuid ( RO): bb0d3098-aea1-45b6-bcde-dfd7cbc34c63 empty ( RO): false device ( RO): xvda userdevice ( RW): 0 mode ( RW): RW type ( RW): Disk attachable ( RO): true storage-lock ( RO): falseuuid ( RO) : 816665c0-9f19-a16d-7ed5-97dfe377e454 vdi-uuid ( RO): &lt;not in database&gt; empty ( RO): true device ( RO): userdevice ( RW): 3 mode ( RW): RO type ( RW): CD attachable ( RO): true storage-lock ( RO): falseChecking to see whether VM can boot on each hostxenserver-200 : OK xenserver-201: OKVM is agile. 根据虚拟机重启优先级，计算资源池允许最大故障数1xe pool-ha-compute-hypothetical-max-host-failures-to-tolerate vm-uuid=&lt;VM_UUID&gt; restart-priority=&lt;restart/best-effort/""&gt; 例如：12[root@node200 ~]# xe pool-ha-compute-hypothetical-max-host-failures-to-tolerate vm-uuid=0ec2a592-29ad-efe1-6ac7-94cafb50d37e restart-priority=restart1 设置受保护的VM1[root@node200 ~]# xe vm-param-set uuid=&lt;VM_UUID&gt; ha-restart-priority=&lt;restart/best-effort/""&gt; ha-always-run=&lt;true/false&gt; VM_UUID：虚拟机UUID restart/best-effort/&quot;&quot;：重启/资源满足时重启/不重启 true/false：是否总是运行状态 例如：1[root@node200 ~]# xe vm-param-set uuid=0ec2a592-29ad-efe1-6ac7-94cafb50d37e ha-restart-priority=restart ha-always-run=true 获取资源池当前配置下，允许的最大故障主机数故障主机超过这个数目，资源池将不足运行池内受保护的虚拟机12[root@node200 ~]# xe pool-ha-compute-max-host-failures-to-tolerate1 设置资源池允许的故障主机数（不能大于允许的最大故障主机数）1xe pool-param-set ha-host-failures-to-tolerate=&lt;num&gt; uuid=&lt;POOL_UUID&gt; num：数字，不能大于允许的最大故障主机数 POOL_UUID：资源池UUID 例如：1[root@node200 ~]# xe pool-param-set ha-host-failures-to-tolerate=1 uuid=53e9df09-bb99-3014-ffdf-3a781a9a84d2 启用资源池高可用1xe pool-ha-enable heartbeat-sr-uuids=&lt;SR_UUID&gt; SR_UUID为信号检测存储池的UUID ha-config:timeout可以配置超时检测，默认为120s 例如：1[root@node200 ~]# xe pool-ha-enable heartbeat-sr-uuids=6e336c82-acc1-8858-1060-32cdc7016804 ha-config:timeout=180 禁用HA解除VM的高可用保护1[root@node200 ~]# xe vm-param-set uuid=&lt;VM_UUID&gt; ha-always-run=&lt;true/false&gt; VM_UUID：虚拟机UUID true/false：是否总是运行状态 例如：1[root@node200 ~]# xe vm-param-set uuid=0ec2a592-29ad-efe1-6ac7-94cafb50d37e ha-always-run=false 禁用资源池高可用1[root@node200 ~]# xe pool-ha-disable]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（八）——V2V服务端分析之Converter]]></title>
      <url>%2F2017%2F03%2F08%2Fv2v-8%2F</url>
      <content type="text"><![CDATA[Converter组件在客户端界面操作转换虚拟机时，每个需要转换的VMware虚拟机都会创建一条job数据存储到job文件中。 Converter是转换服务的核心组件。转换过程是： 获取配置文件信息 加载job文件数据 Quartz调度获取job信息，立即执行调度 获取VMware信息，创建XenServer虚拟机，创建VBD、VDI 将之前创建的VDI卸载 VDI挂载给服务器端VM 下载VMware虚拟机磁盘数据 将VDI从服务端VM上卸载 将VDI挂载给导入的虚拟机 整个过程会实时更新job状态与进度，所有的VDI数据下载完成并挂载给导入的XenServer虚拟机之后，转换操作才结束。 ConversionMgrConversionMgr初始化Initialize方法，调用JobManager的Initialize方法。12345678910public void Initialize()&#123; LOG.Info(&quot;Conversion mgr initializing...&quot;); //读取配置信息 this.ReadConfigurations(); //JobManager初始化 this.m_jobMgr.Initialize(); this.m_bInitialize = true; LOG.Info(&quot;Initialization complete.&quot;);&#125; JobManagerJobManager初始化方法，使用了Quartz框架来调度任务。客户端转换VM会生成job数据，保存到JobStore文件。 123456789101112131415161718192021222324252627282930313233 public void Initialize()&#123; LOG.Info(&quot;Bringing up job manager...&quot;); if (this.m_jobScheduler != null) &#123; throw new InvalidOperationException(Messages.EXCEPTION_ALREADY_INITIALIZED); &#125; //读取配置信息 this.ReadConfigurations(); //加载Job数据文件 this.m_jobStore = JobStore.Load(this.m_dbFile); //Quartz调度框架 /** QUARTZ.NET 是一个定时任务框架 概念： 计划者：IScheduler; 工作 IJob; 触发器：Trigger; 我们先得到一个计划者(IScheduler) 然后创建一个任务工作(IJob)，把这个任务扔给计划者，并且告诉它在什么条件下（触发器Trigger）做这件事 将要定时执行的任务的代码写到IJob接口的Execute方法中即可，时间到来的时候Execute方法会被调用 CrondTrigger是通过Crond表达式设置的触发器，还有SimpleTrigger等简单的触发器， 可以通过TriggerUtils的MakeDailyTrigger（每天执行一次），MakeHourlyTrigger（每小时执行一次） 等方法简化调用。 */ ISchedulerFactory factory = new StdSchedulerFactory(); //获取计划者 this.m_jobScheduler = factory.GetScheduler(); //添加调度任务监听 this.m_jobListener.JobCompletedEvent += new EventHandler&lt;JobCompletedEventArgs&gt;(this.JobCompletedEvent); this.m_jobScheduler.AddGlobalJobListener(this.m_jobListener); this.m_autoShutdownTimer = new Timer((double) (this.m_autoShutdownDelay * 0x3e8)); this.m_autoShutdownTimer.Elapsed += new ElapsedEventHandler(JobManager.OnShutdownTimer); //启动调度 this.m_jobScheduler.Start(); LOG.Info(&quot;Job scheduler started.&quot;); //job调度 this.ResubmitJobsAtStartup();&#125; ResubmitJobsAtStartup方法调用RunJob方法。在RunJob方法中，触发ConversionJob类型的job。1234567JobDetail jobDetail = new JobDetail(obj3.Id, &quot;g1&quot;, typeof(ConversionJob));jobDetail.JobDataMap[&quot;JobObject&quot;] = obj3;Interlocked.Increment(ref this.m_concurrentJobs);//立即执行Trigger trigger = TriggerUtils.MakeImmediateTrigger(0, TimeSpan.Zero);trigger.Name = obj3.Id;this.m_jobScheduler.ScheduleJob(jobDetail, trigger); ConversionJobConversionJob继承抽象类JobBase。JobBase实现接口IJob，实现了Execute方法。12345678910111213141516171819202122public void Execute(JobExecutionContext context)&#123; LOG.Debug(&quot;Execute()&quot;); this.m_job = (JobObject) context.JobDetail.JobDataMap[&quot;JobObject&quot;]; if (this.m_job == null) &#123; LOG.Error(&quot;NULL job object!!&quot;); &#125; else &#123; m_jobContext = this.m_job; try &#123; //Run抽象方法，由子类重写实现 this.Run(context); &#125; finally &#123; m_jobContext = null; &#125; &#125;&#125; ConversionJob类重写Run方法。调用ConvertVM方法。1234567891011121314151617public override void Run(JobExecutionContext context)&#123; Thread.CurrentThread.Priority = ThreadPriority.Lowest; if (!base.m_job.IsRemoved) &#123; this.m_vmImportInfo = new VmImportInfo(); this.m_vmImportInfo.SRuuidToUse = base.m_job.SRUuid; JobInstance jobInstanceData = base.m_job.GetJobInstanceData(); try &#123; //更新Job状态为执行中 base.m_job.UpdateJobState(JobState.Running); //开始转换虚拟机 this.ConvertVM(base.m_job); //更新job状态为完成 base.m_job.UpdateJobState(JobState.Completed); &#125; ConvertVM方法开始虚拟机转换。导出VMware虚拟机信息，导入XenServer虚拟机。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485private void ConvertVM(JobObject job)&#123; &lt;ConvertVM&gt;c__AnonStorey0 storey = new &lt;ConvertVM&gt;c__AnonStorey0(); DateTime now = DateTime.Now; storey.instance = job.GetJobInstanceData(); object[] args = new object[] &#123; JobBase.JobId, storey.instance.JobInfo.SourceVmName, storey.instance.JobInfo.Source.Hostname, base.m_job.XenHost, DateTime.Now.ToString(&quot;r&quot;) &#125;; JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Converting VM &#123;1&#125; from source &#123;2&#125; to dest &#123;3&#125; started at &#123;4&#125; ...&quot;, args); ViVmService service = new ViVmService(); XenVmService service2 = new XenVmService(); //导入状态事件监听 service2.ImportStatusUpdateEvent += new EventHandler&lt;ImportStatusEventArgs&gt;(this.OnImportStatusUpdateEvent); VMExportInfo exportedInfo = null; ViVm vM = null; try &#123; JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Connecting to XenServer at &#123;1&#125; ...&quot;, JobBase.JobId, job.XenHost); //更新job状态为连接XenServer主机 job.UpdateJobStatus(string.Format(Messages.JOB_STATUS_CONNECTING_TO_XS, &quot;XenServer&quot;, job.XenHost), 0L, 0L); //连接XenServer主机 service2.Connect(new Uri(job.XenHost), job.XenUser, job.XenPwd); //更新job进度为5% job.UpdateJobPercentageComplete(5L); JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Connecting to ESX/VirtualCenter at &#123;1&#125;...&quot;, JobBase.JobId, storey.instance.JobInfo.Source.Hostname); //更新job状态为连接VMware主机 job.UpdateJobStatus(string.Format(Messages.JOB_STATUS_CONNECTING_TO_VCENTER, storey.instance.JobInfo.Source.Hostname), 0L, 0L); //连接VMware service.Connect(storey.instance.JobInfo.Source.Hostname, storey.instance.JobInfo.Source.Username, storey.instance.JobInfo.Source.Password); //更新job进度为10% job.UpdateJobPercentageComplete(10L); JobBase.LOG.InfoFormat(&quot;START: find VM object...&quot;, new object[0]); //根据job中的虚拟机uuid获取VMware虚拟机，返回ViVm对象 vM = service.GetVM(storey.instance.JobInfo.SourceVmUUID); if (vM == null) &#123; vM = service.GetVMs().Find(new Predicate&lt;ViVm&gt;(storey.&lt;&gt;m__0)); if (vM == null) &#123; throw new Exception(string.Format(Messages.EXCEPTION_FAILED_TO_FIND_SOURCE_VM, storey.instance.JobInfo.SourceVmName)); &#125; &#125; JobBase.LOG.InfoFormat(&quot;DONE: finding VM object&quot;, new object[0]); //更新job进度为15% job.UpdateJobPercentageComplete(15L); this.m_vmImportInfo.IsTemplate = vM.IsTemplate; //判断是否保留MAC地址 if (job.GetJobInstanceData().JobInfo.PreserveMAC.HasValue) &#123; this.m_vmImportInfo.preserveMAC = job.GetJobInstanceData().JobInfo.PreserveMAC.Value; &#125; else &#123; this.m_vmImportInfo.preserveMAC = false; &#125; this.m_vmImportInfo.networkMappings = job.GetJobInstanceData().JobInfo.NetworkMappings; job.VMTotalBytes = vM.CommitedStorage; if (this.m_vmImportInfo.IsTemplate) &#123; object[] objArray2 = new object[] &#123; JobBase.JobId, storey.instance.JobInfo.SourceVmName, job.VMTotalBytes, now.ToShortTimeString() &#125;; JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Exporting VM Template&apos;&#123;1&#125;&apos; with &#123;2&#125; bytes at &#123;3&#125;&quot;, objArray2); &#125; else &#123; object[] objArray3 = new object[] &#123; JobBase.JobId, storey.instance.JobInfo.SourceVmName, job.VMTotalBytes, now.ToShortTimeString() &#125;; JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Exporting VM &apos;&#123;1&#125;&apos; with &#123;2&#125; bytes at &#123;3&#125;&quot;, objArray3); &#125; //更新job状态为开始导出VMware虚拟机 job.UpdateJobStatus(Messages.JOB_STATUS_EXPORTING_VM, 0L, 0L); //调用ViVm的BeginExport方法，返回导出数据信息（包括虚拟机名称、网络信息、MAC地址、导出设备信息、OVF信息等） exportedInfo = vM.BeginExport(); //XenServer导入虚拟机 service2.ImportVM(exportedInfo, this.m_vmImportInfo); &#125; finally &#123; if ((vM != null) &amp;&amp; (exportedInfo != null)) &#123; vM.EndExport(exportedInfo); &#125; service.Disconnect(); service2.Disconnect(); DateTime time3 = DateTime.Now; object[] objArray4 = new object[] &#123; JobBase.JobId, storey.instance.JobInfo.SourceVmName, time3.ToShortTimeString(), (TimeSpan) (time3 - now) &#125;; JobBase.LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Converting VM &apos;&#123;1&#125;&apos; stopped at &#123;2&#125; after a duration of &#123;3&#125;&quot;, objArray4); &#125;&#125; XenVmServiceXenVmService的ImportVM方法。 调用Importer的Process方法， 根据VMware虚拟机OVF等信息创建XenServer虚拟机 执行Process方法之后，调用ImportVMDisks方法，导入虚拟磁盘。1234567891011121314151617181920Importer import = new Importer(); XenVm importedVm = null; try &#123; import.Process(this.m_session, ovfEnvelope, vmImportInfo.SRuuidToUse, vmImportInfo.preserveMAC, exportedInfo.vmNetworkMACInfo, vmImportInfo.networkMappings, vmImportInfo.IsTemplate); if (JobBase.IsRemoved) &#123; e.currentTaskDesc = Messages.TASK_DESC_JOB_CANCEL_PROCESSED; this.ImportStatusUpdateEvent(this, e); LOG.WarnFormat(&quot;&lt;&#123;0&#125;&gt;: Cancelling running job during meta import for VM &#123;1&#125;&quot;, JobBase.JobId, exportedInfo.VmName); throw new UserAbortException(string.Format(Messages.USER_ABORT_EXCEPTION_CANCEL_JOB3, exportedInfo.VmName)); &#125; &#125;........//导入虚拟磁盘this.ImportVMDisks(importedVm, import, exportedInfo, vmImportInfo);........ ImporterProcess方法，调用AddResourceSettingData方法添加VBD与VDI。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void Process(Session xenSession, EnvelopeType ovfEnvelope, string srUuid, bool preserveMAC, List&lt;VMNetworkMACInfo&gt; vmNetworkMACInfo, Hashtable networkMappings, bool isTemplate)&#123; if (xenSession == null) &#123; throw new InvalidOperationException(string.Format(Messages.ERROR_NOT_CONNECTED, &quot;XenServer&quot;)); &#125; this.m_ovfEnv = ovfEnvelope; this.m_importedVMInfo = new ImportedVMInfo(); this.m_importedVMInfo.disksInfo = new List&lt;ImportedDiskInfo&gt;(); this.vifDeviceIndex = 0; if (ovfEnvelope.Item is VirtualSystem_Type) &#123; VirtualSystem_Type item = (VirtualSystem_Type) ovfEnvelope.Item; ovfEnvelope.Item = new VirtualSystemCollection_Type(); ((VirtualSystemCollection_Type) ovfEnvelope.Item).Content = new Content_Type[] &#123; item &#125;; &#125; VirtualHardwareSection_Type system = null; foreach (VirtualSystem_Type type3 in ((VirtualSystemCollection_Type) ovfEnvelope.Item).Content) &#123; system = this.ovfImport.FindVirtualHardwareSectionByAffinity(ovfEnvelope, type3.id, &quot;xen&quot;); string ovfName = this.ovfImport.FindSystemName(ovfEnvelope, type3.id); //创建虚拟机 XenRef&lt;VM&gt; vmRef = this.DefineSystem(xenSession, system, ovfName); if (vmRef == null) &#123; throw new ImportException(Messages.ERROR_DEFINE_SYSTEM); &#125; this.m_importedVMInfo.vmInfo = VM.get_record(xenSession, (string) vmRef); this.SetDeviceConnections(ovfEnvelope, system); if (isTemplate) &#123; VM.set_is_a_template(xenSession, (string) vmRef, true); &#125; try &#123; foreach (RASD_Type type4 in system.Item) &#123; if (((type4.ResourceType.Value == 0x11) || (type4.ResourceType.Value == 0x13)) || (type4.ResourceType.Value == 0x15)) &#123; if (OVF.ValidateProperty(&quot;Caption&quot;, type4) &amp;&amp; ((type4.Caption.Value.ToUpper().Contains(&quot;COM&quot;) || type4.Caption.Value.ToUpper().Contains(&quot;FLOPPY&quot;)) || type4.Caption.Value.ToUpper().Contains(&quot;ISO&quot;))) &#123; continue; &#125; this.SetIfDeviceIsBootable(ovfEnvelope, type4); &#125; string compressed = &quot;None&quot;; //添加资源，VBD与VDI this.AddResourceSettingData(xenSession, vmRef, type4, this.ovfImport.FindRasdFileName(ovfEnvelope, type4, out compressed), srUuid, preserveMAC, vmNetworkMACInfo, networkMappings); &#125; &#125; catch (Exception exception) &#123; LOG.ErrorFormat(&quot;Importer: Failed processing device types in OVF file: Error &apos;&#123;0&#125;&apos;.&quot;, exception.Message); throw exception; &#125; &#125;&#125; 调用ImportVMDisks方法 将之前创建的VDI卸载 VDI挂载给服务器端VM 下载VMware虚拟机磁盘数据 将VDI从服务端VM上卸载 将VDI挂载给导入的虚拟机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 private void ImportVMDisks(XenVm importedVm, Importer import, VMExportInfo exportedInfo, VmImportInfo importInfo)&#123; ImportStatusEventArgs e = new ImportStatusEventArgs(Messages.IMPORTING_VM_DISKS_STATUS, string.Empty, 0L, 0L, 0L); this.ImportStatusUpdateEvent(this, e); foreach (ImportedDiskInfo info in import.ImportedVMInfo.disksInfo) &#123; VBD vbdInfo = null; try &#123; if (JobBase.IsRemoved) &#123; e.currentTaskDesc = Messages.TASK_DESC_JOB_CANCEL_PROCESSED; this.ImportStatusUpdateEvent(this, e); LOG.WarnFormat(&quot;&lt;&#123;0&#125;&gt;: Cancelling running job during disk import for VM &#123;1&#125;&quot;, JobBase.JobId, importedVm.Name); throw new UserAbortException(string.Format(Messages.USER_ABORT_EXCEPTION_CANCEL_JOB2, importedVm.Name)); &#125; LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Detaching VDI &#123;1&#125; from imported VM &#123;2&#125;.&quot;, JobBase.JobId, info.vdiInfo.uuid, importedVm.Name); //VBD unplug（之前创建的VDI是没有plug给VM的，这里做一下unplug操作，以防万一被plug） importedVm.DetachDisk(info.vbdInfo); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Attaching VDI &#123;1&#125; to this appliance.&quot;, JobBase.JobId, info.vdiInfo.uuid); //将VDI plug给服务端VM vbdInfo = this.m_conversionVM.AttachDisk(info.vdiInfo); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Newly-attached VBD disk on this appliance &#123;1&#125;.&quot;, JobBase.JobId, vbdInfo.device); File_Type type = import.Ovf.FindFileReference(import.Envelope, info.OvfDiskInfo.fileRef); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Searching for disk key &#123;1&#125;.&quot;, JobBase.JobId, type.href); VMExportDeviceInfo devInfo = this.FindExportDeviceInfo(exportedInfo, type.href); if (devInfo != null) &#123; LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Imported disk key=&#123;1&#125; match to exported key=&#123;2&#125; &quot;, JobBase.JobId, type.href, devInfo.Key); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Streaming from url=&#123;1&#125; to device=&#123;2&#125;...&quot;, JobBase.JobId, devInfo.Url, vbdInfo.device); //下载VMware磁盘数据 this.DownloadDisk(exportedInfo, devInfo, vbdInfo, JobBase.JobId); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Streaming done for device=&#123;1&#125;.&quot;, JobBase.JobId, vbdInfo.device); &#125; else &#123; LOG.ErrorFormat(&quot;&lt;&#123;0&#125;&gt;: No match key found for imported disk key=&#123;1&#125;!!&quot;, JobBase.JobId, type.href); throw new InvalidOperationException(string.Format(Messages.EXCEPTION_NO_MATCH_KEY_FOUND, type.href)); &#125; LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Detaching device=&#123;1&#125; from appliance.&quot;, JobBase.JobId, vbdInfo.device); //将VDI从服务端VM上unlpug this.m_conversionVM.DetachDisk(vbdInfo); LOG.InfoFormat(&quot;&lt;&#123;0&#125;&gt;: Reattaching device=&#123;1&#125; to imported VM.&quot;, JobBase.JobId, info.vbdInfo.userdevice); //将VDI挂载给导入的虚拟机 importedVm.ReattachDisk(info.vbdInfo); &#125; catch &#123; if (vbdInfo != null) &#123; this.m_conversionVM.DestroyDisk(vbdInfo); &#125; throw; &#125; &#125;&#125; DownloadDisk方法下载VMware磁盘数据，使用/opt/vpxxcm/conversion/vdiskget来执行1234567891011121314151617private void DownloadDisk(VMExportInfo exportedInfo, VMExportDeviceInfo devInfo, VBD vbdInfo, string jobId)&#123; string str = XenVmDiskPath.FixupDiskPath(vbdInfo.device); string str2 = string.Format(&quot;/dev/&#123;0&#125;&quot;, str); int num = -1; for (int i = 0; i &lt; Settings.Default.DownloadRetryCount; i++) &#123; string[] arguments = new string[] &#123; Settings.Default.DownloadOptions, devInfo.Url, str2, jobId, this.Uri.ToString() &#125;; num = this.RunCancelableJobProcess(&quot;/opt/vpxxcm/conversion/vdiskget&quot;, Settings.Default.DownloadTimeoutMs, Settings.Default.DownloadPeriodMs, arguments); if (num == 0) &#123; return; &#125; Thread.Sleep(Settings.Default.DownloadRetryDelayMs); &#125; throw new Exception(string.Format(&quot;Failed to download disk from &#123;0&#125; to &#123;1&#125;. (vdiskget result = &#123;2&#125;)&quot;, devInfo.Url, str2, num));&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（七）——V2V服务端分析之convsvc]]></title>
      <url>%2F2017%2F03%2F07%2Fv2v-7%2F</url>
      <content type="text"><![CDATA[V2V转换器服务端是运行在XenServer Conversion Manager Virtual Appliance上，主要有以下组件： convsvc：主服务，Windows服务程序 Converter：转换的核心组件 Citrix.ExportImport.CommonTypes：公共类库 VimService：VMware API Quartz：调度组件 XenServer：XAPI convsvc服务convsvc服务是运行在V2V转换器的服务端XenServer Conversion Manager Virtual Appliance上，作为Windows Service在mono中运行。 主要有两个类： ConversionSvc ConversionSvcWrapper ConversionSvcWrapperConversionSvcWrapper类继承ServiceBase，重写了OnStart、OnStop、OnShutdown方法，是ConversionSvc服务的包装类，可以把ConversionSvc作为一个Windows服务程序来运行。 ConversionSvcConversionSvc类定义了获取VMware虚拟机、网络、磁盘空间，以及job、log等操作的一些方法。实际上是对外开放了这些接口。 代码分析ConversionSvcWrapper的Main方法作为Windows服务程序的入口。12345678Console.WriteLine(&quot;Starting conversion service in interactive mode&quot;);//调用OnStart方法wrapper.OnStart(args);Console.WriteLine(&quot;Prese any key and &lt;Enter&gt; to exit&quot;);Console.ReadLine();wrapper.OnStop();Console.ReadLine();Console.WriteLine(&quot;Exiting...&quot;); 重写的OnStart方法。12345678910111213protected override void OnStart(string[] args)&#123; LOG.Info(&quot;Starting service...&quot;); try &#123; //调用ConversionSvc的Start方法 this.m_service.Start(); &#125; catch (Exception exception) &#123; throw new Exception(&quot;Failed to start service.&quot;, exception); &#125;&#125; ConversionSvc的Start方法。ConversionMgr是Converter组件的类。1234567891011public void Start()&#123; LOG.Info(&quot;Starting the conversion service...&quot;); //获取配置文件中的配置信息 this.ReadConfigurations(); //初始化ConversionMgr ConversionMgr.Instance.Initialize(); //HTTP监听线程启动 this.m_listener.Start(); this.m_listenerThread.Start();&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Git GUI提交代码到GitHub]]></title>
      <url>%2F2017%2F03%2F07%2Fgit-gui-1%2F</url>
      <content type="text"><![CDATA[使用Git GUI提交代码到GitHub，主要有以下步骤： GitHub创建仓库 clone远程仓库 复制代码 Git GUI操作 下面以XenCenter为例，环境： Windows 7 x64 Git for Windows(Git-2.11.0-64-bit.exe) GitHub创建仓库先在github上新建一个仓库XenCenter，仓库的地址为：https://github.com/hl10502/XenCenter.git clone远程仓库在Git GUI上clone远程仓库到本地 复制代码复制XenCenter代码到Target Directory。比如XenCenter的代码目录为E:\visual studio 2010\Projects\xenadmin-master7.0\xenadmin Git GUI操作Git GUI操作提交代码。 Rescan:重新扫描，显示本地有新增、修改的代码文件 Stage Changed：阶段提交，显示需要提交的文件 Commit：提交到本地 Push：上传到GitHub]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（六）——Unable to Boot VMware SCSI Disk]]></title>
      <url>%2F2017%2F03%2F07%2Fv2v-6%2F</url>
      <content type="text"><![CDATA[Unable to Boot VMware SCSI Disk如果VMware VM从SCSI磁盘引导，但还包含一个或多个IDE硬盘，当做V2V转换成功到XenServer上之后，则VM可能无法启动。 这是因为迁移过程IDE硬盘分配的设备号比SCSI磁盘小，但是XenServer从分配给设备0的硬盘启动VM，导致VM启动失败。要解决此问题，需要修改VM的虚拟磁盘位置，以便正确引导系统盘启动VM。 有两种方法： 使用CLI命令行 使用XenCenter界面 使用CLI根据vmUuid找到VBD 1xe vbd-list vm-uuid=5be02646-d70e-ed83-f68b-286bad3890ba 设置系统盘的VBD为device 0 与可引导 12xe vbd-param-set userdevice=0 uuid=873f1c20-212e-913a-50fd-5352f81478dbxe vbd-param-set bootable=true uuid=873f1c20-212e-913a-50fd-5352f81478db XenCenter操作选中一个存储池，在该存储池的“存储”选项，选择虚拟磁盘，查看属性，修改虚拟磁盘的“设备位置”为0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C#反编译.resources资源文件]]></title>
      <url>%2F2017%2F02%2F28%2Fvs2013-decompile%2F</url>
      <content type="text"><![CDATA[C#反编译的.resources资源文件，使用resgen命令生成.resx文件在项目中使用C#的.dll文件反编译出来的.resources资源文件，需要生成.resx才能在项目中使用。 打开VS2013命令行 执行resgen命令生成resx文件将资源文件copy到命令目录下（D:\Program Files (x86)\Microsoft Visual Studio 12.0\VC），执行命令 resgen xx.resources xx.resx。 例如WinAPI.FriendlyErrorNames.resources文件 修改resx文件例如：生成的.resx文件为WinAPI.FriendlyErrorNames.resx。找到项目中的FriendlyErrorNames.cs文件，WinAPI为namespace，FriendlyErrorNames.cs文件中资源为WinAPI.FriendlyErrorNames。 重命名WinAPI.FriendlyErrorNames.resx为FriendlyErrorNames.resx，并将FriendlyErrorNames.resx文件copy到项目的WinAPI目录中（与FriendlyErrorNames.cs同一目录）。 注意如下代码的资源文件的路径为”WinAPI.FriendlyErrorNames”。一般加载不了资源可能是路径写错。12345678910111213[EditorBrowsable(EditorBrowsableState.Advanced)] public static System.Resources.ResourceManager ResourceManager &#123; get &#123; if (object.ReferenceEquals(resourceMan, null)) &#123; System.Resources.ResourceManager manager = new System.Resources.ResourceManager(&quot;WinAPI.FriendlyErrorNames&quot;, typeof(FriendlyErrorNames).Assembly); resourceMan = manager; &#125; return resourceMan; &#125; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（五）——ERROR Credentials for user 'root' are invalid on http://169.254.0.1]]></title>
      <url>%2F2017%2F02%2F28%2Fv2v-5%2F</url>
      <content type="text"><![CDATA[ERROR Credentials for user ‘root’ are invalid on http://169.254.0.1XCM所在的XenServer主机的iptables阻止XCM的eth0通过dhcp获取ip，需要关闭XenServer的防火墙。 错误分析XCM的log。 123456-bash-4.2# cat /var/log/conversion/convsvc.log2017-01-13 08:48:03,042 ERROR Credentials for user 'root' are invalid on http://169.254.0.1.2017-01-13 08:48:03,066 ERROR System.InvalidOperationException: Cannot be changed after headers are sent. at System.Net.HttpListenerResponse.set_StatusCode (Int32 value) [0x00000] in &lt;filename unknown&gt;:0 at CookComputing.XmlRpc.XmlRpcListenerService.ProcessRequest (System.Net.HttpListenerContext RequestContext) [0x00000] in &lt;filename unknown&gt;:0 at convsvc.ConversionSvc+&lt;HandleNewContext&gt;c__AnonStorey0.&lt;&gt;m__0 (System.Object state) [0x00000] in &lt;filename unknown&gt;:0 启动XCM的网络失败，eth0不能动态生成IP地址。 123456789-bash-4.2# service network restartShutting down interface eth0: [ OK ]Shutting down interface eth1: [ OK ]Shutting down loopback interface: [ OK ]Bringing up loopback interface: [ OK ]Bringing up interface eth0:Determining IP information for eth0... failed. [FAILED]Bringing up interface eth1: [ OK ] 查看XCM的日志，dhcp接收不到数据。 123456-bash-4.2# tail -f /var/log/messagesJan 13 09:08:28 xcm dhclient: DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 10 (xid=0x15d3da80)Jan 13 09:08:38 xcm dhclient: DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 17 (xid=0x15d3da80)Jan 13 09:08:55 xcm dhclient: DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 15 (xid=0x15d3da80)Jan 13 09:09:10 xcm dhclient: DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 13 (xid=0x15d3da80)Jan 13 09:09:23 xcm dhclient: No DHCPOFFERS received. 查看XenServer主机的iptables。 12345678910111213141516171819202122232425262728293031[root@xenserver113 ~]# iptables -LChain INPUT (policy ACCEPT)target prot opt source destinationRH-Firewall-1-INPUT all -- anywhere anywhereChain FORWARD (policy ACCEPT)target prot opt source destinationRH-Firewall-1-INPUT all -- anywhere anywhereChain OUTPUT (policy ACCEPT)target prot opt source destinationChain RH-Firewall-1-INPUT (2 references)target prot opt source destinationACCEPT all -- anywhere anywhereACCEPT icmp -- anywhere anywhere icmp anyACCEPT esp -- anywhere anywhereACCEPT ah -- anywhere anywhereACCEPT udp -- anywhere 224.0.0.251 udp dpt:mdnsACCEPT udp -- anywhere anywhere udp dpt:ippACCEPT tcp -- anywhere anywhere tcp dpt:ippACCEPT all -- anywhere anywhere ctstate RELATED,ESTABLISHEDACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:sshACCEPT udp -- anywhere anywhere ctstate NEW udp dpt:ha-clusterACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:httpACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:httpsACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:4430ACCEPT udp -- anywhere anywhere ctstate NEW udp dpt:ntpACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:ndmpACCEPT tcp -- anywhere anywhere ctstate NEW tcp dpt:vceREJECT all -- anywhere anywhere reject-with icmp-host-prohibited 查看XenServer主机的iptables的状态。 1234567891011121314151617181920212223242526272829303132[root@xenserver113 ~]# service iptables statusTable: filterChain INPUT (policy ACCEPT)num target prot opt source destination1 RH-Firewall-1-INPUT all -- 0.0.0.0/0 0.0.0.0/0Chain FORWARD (policy ACCEPT)num target prot opt source destination1 RH-Firewall-1-INPUT all -- 0.0.0.0/0 0.0.0.0/0Chain OUTPUT (policy ACCEPT)num target prot opt source destinationChain RH-Firewall-1-INPUT (2 references)num target prot opt source destination1 ACCEPT all -- 0.0.0.0/0 0.0.0.0/02 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 icmptype 2553 ACCEPT esp -- 0.0.0.0/0 0.0.0.0/04 ACCEPT ah -- 0.0.0.0/0 0.0.0.0/05 ACCEPT udp -- 0.0.0.0/0 224.0.0.251 udp dpt:53536 ACCEPT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:6317 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:6318 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED9 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:2210 ACCEPT udp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW udp dpt:69411 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:8012 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:44313 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:443014 ACCEPT udp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW udp dpt:12315 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:1000016 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 ctstate NEW tcp dpt:1111117 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited 解決方法关闭XenServer主机的iptables，再次重启XCM的网络。 12345[root@xenserver113 ~]# service iptables stopiptables: Flushing firewall rules: [ OK ]iptables: Setting chains to policy ACCEPT: filter [ OK ]iptables: Unloading modules: [ OK ] 123456789[root@xenserver113 ~]# iptables -LChain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destination 重启XCM的网络。 123456789-bash-4.2# service network restartShutting down interface eth0: [ OK ]Shutting down interface eth1: [ OK ]Shutting down loopback interface: [ OK ]Bringing up loopback interface: [ OK ]Bringing up interface eth0:Determining IP information for eth0... done. [ OK ]Bringing up interface eth1: [ OK ] 查看网络，eth0生成动态的IP。 12345678910-bash-4.2# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000 link/ether 32:d9:a4:db:02:8e brd ff:ff:ff:ff:ff:ff inet 169.254.0.2/16 brd 169.254.255.255 scope global eth03: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000 link/ether 46:71:b8:a5:fd:07 brd ff:ff:ff:ff:ff:ff inet 192.168.217.199/24 brd 192.168.217.255 scope global eth1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（四）——XenServer Conversion Manager Virtual Appliance虚拟机网络修改]]></title>
      <url>%2F2017%2F02%2F28%2Fv2v-4%2F</url>
      <content type="text"><![CDATA[XenServer Conversion Manager Virtual Appliance网络配置XenServer Conversion Manager Virtual Appliance上有两张网卡。后期由于网络环境变化或者制作模板导入是需要修改网络信息的。 eth0：dhcp方式动态获取，在虚拟化层面使用XenServer主机的内部管理网络，IP地址动态获取为169.254.0.x eth1：static静态配置 查看虚拟机使用的network在主机上查询虚拟机。 1234[root@xenserver113 ~]# xe vm-list name-label="Citrix XCM Virtual Appliance"uuid ( RO) : 522e1ca7-0c99-6090-b0f1-ec4439254286 name-label ( RW): Citrix XCM Virtual Appliance power-state ( RO): running 查看虚拟机的VIF。 1234567891011[root@xenserver113 ~]# xe vif-list vm-uuid=522e1ca7-0c99-6090-b0f1-ec4439254286uuid ( RO) : f0c33a41-f240-9a9f-37fb-3188e3b27358 vm-uuid ( RO): 522e1ca7-0c99-6090-b0f1-ec4439254286 device ( RO): 1 network-uuid ( RO): c73b7b36-e65f-0cf7-0183-3bfdbfed0df0uuid ( RO) : 39ec0a7a-35e6-21ff-085b-371a6e019ef0 vm-uuid ( RO): 522e1ca7-0c99-6090-b0f1-ec4439254286 device ( RO): 0 network-uuid ( RO): 33d92b86-dfc7-950c-51ee-6dc3f9f27394 虚拟机的虚拟网络eth0使用的是物理主机的内部管理网络，物理主机的内部管理IP为169.254.0.1。 123456789101112[root@xenserver113 ~]# xe network-param-list uuid=33d92b86-dfc7-950c-51ee-6dc3f9f27394uuid ( RO) : 33d92b86-dfc7-950c-51ee-6dc3f9f27394 name-label ( RW): Host internal management network name-description ( RW): Network on which guests will be assigned a private link-local IP address which can be used to talk XenAPI VIF-uuids (SRO): 03f850b1-f7bb-320d-8d54-aa3d3cf3a102; 39ec0a7a-35e6-21ff-085b-371a6e019ef0 PIF-uuids (SRO): MTU ( RW): 1500 bridge ( RO): xenapi other-config (MRW): is_guest_installer_network: true; is_host_internal_management_network: true; ip_begin: 169.254.0.1; ip_end: 169.254.255.254; netmask: 255.255.0.0 blobs ( RO): tags (SRW): default-locking-mode ( RW): unlocked 修改虚拟机eth1静态网络eth0的网络是不能修改的。 12345-bash-4.2# cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0BOOTPROTO=dhcpONBOOT=yesNOZEROCONF=true 可以修改eth1的网络（IP、子网掩码、网关）。 123456789-bash-4.2# cat /etc/sysconfig/network-scripts/ifcfg-eth1DEVICE=eth1ONBOOT=yesNOZEROCONF=trueBOOTPROTO=noneNETMASK=255.255.255.0IPADDR=192.168.217.222PEERDNS=yesDNS1= 12345-bash-4.2# cat /etc/sysconfig/networkNETWORKING=yesNETWORKING_IPV6=noHOSTNAME=xcmGATEWAY=192.168.217.254]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（三）——XenServer Conversion Manager Virtual Appliance转换服务convsvc配置]]></title>
      <url>%2F2017%2F02%2F28%2Fv2v-3%2F</url>
      <content type="text"><![CDATA[convsvc服务配置修改XenServer Conversion Manager Virtual Appliance上的主要服务是convsvc，它是整个V2V转换的核心。 在XenServer Conversion Manager Virtual Appliance导入到XenServer之后，需要修改/opt/vpxxcm/conversion/convsvc.exe.config文件配置信息。 IP地址不是动态获取，修改ConversionServer.IP将1&lt;add key="ConversionServer.IP" value="[/etc/dhclient-enter-hooks script will fill it in]" /&gt; 修改成1&lt;add key="ConversionServer.IP" value="localhost" /&gt; 增加日志编码支持在12&lt;appender name="RollingLogFileAppender" type="log4net.Appender.RollingFileAppender"&gt;&lt;param name="File" value="/var/log/conversion/convsvc.log" /&gt; 下面增加1&lt;param name="encoding" value="utf-8" /&gt; 修改自动关闭系统时间将1&lt;add key="JobManager.AutoShutdownDelay" value="300" /&gt; &lt;!-- 5分钟 --&gt; 修改成1&lt;add key="JobManager.AutoShutdownDelay" value="86400" /&gt; &lt;!-- 1天 --&gt; 重启服务1-bash-4.2# /etc/init.d/convsvcd restart]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（二）——XenServer Conversion Manager Virtual Appliance虚拟机配置]]></title>
      <url>%2F2017%2F02%2F28%2Fv2v-2%2F</url>
      <content type="text"><![CDATA[XenServer Conversion Manager Virtual ApplianceXenServer Conversion Manager Virtual Appliance是执行转换的虚拟设备，需要导入到XenServer主机或资源池中的master主机。 将XenServer-7.0.0-vpx-conversion.xva导入到XenServer主机中（可以使用WinCenterClient工具导入），配置好网络以及相关配置就可以作为XenServer Conversion Manager Virtual Appliance。 配置密码、主机名、网络输入yes，接受license。 设置密码。 设置hostname。 设置网络信息，不使用DHCP。 最后确定所有的配置，自动重启网络。 修改其他配置（可以不修改）查找含有xenserver字符的文件。 12345678910111213-bash-4.2# cd /etc/init.d/-bash-4.2# grep -in xenserver *convsvcd:26:# Init file for Citrix XenServer VM Conversion serviceconvsvcd:29:# description: Citrix XenServer VM conversion serviceeula_render.sh:6: echo -e "\033[44;37;1mCitrix XenServer Conversion Manager Virtual Appliance - End User License Agreement \033[0m"vpx_startup_setup.sh:19:product="XenServer"vpx_startup_setup.sh:20:banner="\033[44;37;1mCitrix XenServer Conversion Manager Virtual Appliance \033[0m"vpx_startup_setup.sh:39: echo -e "The Citrix XenServer Conversion Manager Service is running at \033[40;33;1m$ip\033[0m"vpx_startup_setup.sh:187:makecert -r -n "CN=Citrix XenServer Conversion Manager Appliance" -sv root.key root.cer &gt;&gt; $logfile 2&gt;&amp;1vpx_startup_setup.sh:250:echo "Citrix XenServer Conversion Manager Virtual Appliance configuration is complete."vpx_startup_setup.sh:257:echo -e "The Citrix XenServer Conversion Manager Service is running at \033[40;33;1m$ip\033[0m"xcm_self_configure.sh:69:makecert -r -n "CN=Citrix XenServer Conversion Manager Appliance" -sv root.key root.cer &gt;&gt; $logfile 2&gt;&amp;1xcm_self_configure.sh:131:echo "Citrix XenServer Conversion Manager Virtual Appliance configuration is complete." 修改Citrix XenServer，使用其他字符替换。xx使用你想替换成的字符。 123456-bash-4.2# sed -i 's/Citrix XenServer/xx/g' vpx_startup_setup.sh-bash-4.2# sed -i 's/Citrix XenServer/xx/g' convsvcd-bash-4.2# sed -i 's/Citrix XenServer/xx/g' xcm_self_configure.sh-bash-4.2# sed -i 's/Citrix XenServer/xx/g' eula_render.sh-bash-4.2# cd /etc/-bash-4.2# sed -i 's/Citrix XenServer/xx/g' issue 重启虚拟机。 1-bash-4.2# reboot]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[V2V转换器（一）——XenServer Conversion Manager]]></title>
      <url>%2F2017%2F02%2F28%2Fv2v-1%2F</url>
      <content type="text"><![CDATA[XenServer Conversion Manager简介XenServer Conversion Manager：简称XCM，是将VMware的虚拟机转换成XenServer上的虚拟机的工具，即是V2V转换器。 XCM分为两个部分： XenServer Conversion Manager Console XenServer Conversion Manager Virtual Appliance XenServer Conversion Manager ConsoleXenServer Conversion Manager Console是Windows下的用户界面，需要连接到XenServer和XenServer Conversion Manager Virtual Appliance。 XenServer Conversion Manager Virtual ApplianceXenServer Conversion Manager Virtual Appliance是执行转换的虚拟设备，需要导入到XenServer主机或资源池中的master主机。 XenServer Conversion Manager Virtual Appliance需要的资源： XenServer 6.1/XenServer 6.2 SP1/XenServer 6.5 SP1/XenServer 7.0 Disk space: 4GB of disk space Memory: 1GB (will use up to 2GB if available) Virtual CPU allocation: 1 vCPU 通过XenServer Conversion Manager Virtual Appliance这台运行在XenServer主机上的虚拟机，将VMware虚拟机转换为XenServer虚拟机格式，并将这些虚拟机导入到XenServer池或主机来完成虚拟机转换。 转换过程XenServer Conversion Manager Console使用一个简单的向导式操作来连接XenServer主机，转换VMware上的多个虚拟机到XenServer。 连接XenServer主机，启动XenServer Conversion Manager Virtual Appliance 连接VMware Server 获取XenServer的本地或共享存储池作为转换的目标存储 获取VMware虚拟机，选择一个或多个需要转换的VMware虚拟机 在VMware和XenServer之间映射网络设置，以便转换的虚拟机启动,并使用正确的网络设置运行 转换虚拟机 XenServer Conversion Manager不会删除或更改现有的VMware环境。虚拟机将复制到XenServer环境中，而不会从VMware中删除。 VMware与XenServerVMware与XenServer对应的术语 XenServer支持版本 XenServer 6.1 XenServer 6.2 Service Pack 1 XenServer 6.5 Service Pack 1 XenServer 7.0 VMware支持版本 vCenter Server 4.0 and 4.1 vSphere 4.0 and 4.1 ESXi 5.0.0, 5.1.0, 5.5.0 and 6.0.0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenCenter分析（二）]]></title>
      <url>%2F2017%2F02%2F27%2Fxencenter-2%2F</url>
      <content type="text"><![CDATA[要使用XenCenter来管理XenServer主机和资源池，以及部署、监视、管理和迁移虚拟机。 首先需要添加XenServer物理主机。 AddServerDialog添加服务器的代码在AddServerDialog.cs中。 添加服务器的对话框“添加”按钮的事件AddButton_Click，调用ConnectToServer方法连接服务器。 1234567891011121314151617181920212223242526272829303132333435363738394041public virtual void AddButton_Click(object sender, EventArgs e)&#123; /** *ip地址与端口，可以一次连接多个主机，以&quot;;&quot;分隔 *一般我们连接一个，直接使用ip，端口默认使用80 */ string hostnameAndPort = ServerNameComboBox.Text.Trim(); //用户名 string username = UsernameTextBox.Text.Trim(); //密码 string password = PasswordTextBox.Text; //多主机 string[] multipleHosts; if (TryGetMultipleHosts(hostnameAndPort, out multipleHosts)) &#123; //循环依次连接 foreach (string h in multipleHosts) &#123; ConnectToServer(null, h, ConnectionsManager.DEFAULT_XEN_PORT, username, password, comboBoxClouds.SelectedItem != null ? comboBoxClouds.SelectedItem.ToString() : string.Empty); &#125; &#125; else &#123; string hostname; int port; if (!StringUtility.TryParseHostname(hostnameAndPort, ConnectionsManager.DEFAULT_XEN_PORT, out hostname, out port)) &#123; //分隔ip与端口 hostname = hostnameAndPort; port = ConnectionsManager.DEFAULT_XEN_PORT; &#125; //连接主机 ConnectToServer(connection, hostname, port, username, password, comboBoxClouds.SelectedItem != null ? comboBoxClouds.SelectedItem.ToString() : string.Empty); &#125; Close();&#125; ConnectToServer方法，经ip、用户名、密码等参数组装XenConnection对象，调用XenConnectionUI的BeginConnect方法。 123456789101112131415161718192021222324protected void ConnectToServer(IXenConnection conn, string hostname, int port, string username, string password, string version) &#123; if (conn == null) &#123; XenConnection connection = new XenConnection(); connection.CachePopulated += conn_CachePopulated; connection.fromDialog = true; conn = connection; &#125; else if (!_changedPass) &#123; conn.EndConnect(); // in case we&apos;re already connected &#125; conn.Hostname = hostname; conn.Port = port; conn.Username = username; conn.Password = password; conn.ExpectPasswordIsCorrect = false; conn.Version = version; if (!_changedPass) XenConnectionUI.BeginConnect(conn, true, Owner, false); &#125; XenConnectionUI的BeginConnect方法，调用XenConnection的BeginConnect方法。 123456789101112131415161718192021222324public static void BeginConnect(IXenConnection connection, bool interactive, Form owner, bool initiateMasterSearch) &#123; Program.AssertOnEventThread(); //注册连接事件，连接结果与连接状态的处理 RegisterEventHandlers(connection); if (interactive) &#123; // CA-214953 - Focus on this connection&apos;s dialog, if one exists, otherwise create one ConnectingToServerDialog dlg; if (connectionDialogs.TryGetValue(connection, out dlg)) &#123; UnregisterEventHandlers(connection); if (dlg.WindowState == FormWindowState.Minimized) dlg.WindowState = FormWindowState.Normal; dlg.Focus(); return; &#125; dlg = new ConnectingToServerDialog(connection); connectionDialogs.Add(connection, dlg); dlg.BeginConnect(owner, initiateMasterSearch); &#125; else ((XenConnection)connection).BeginConnect(initiateMasterSearch, PromptForNewPassword); &#125; XenConnection的BeginConnect方法,开启新线程处理连接 1234567891011121314//清除事件队列ClearEventQueue();OnBeforeMajorChange(false);Cache.Clear();OnAfterMajorChange(false);//设置连接任务对象connectTask = new ConnectTask(Hostname, Port);StopMonitor();heartbeat = new Heartbeat(this, XenAdminConfigManager.Provider.ConnectionTimeout);//开启连接工作线程Thread t = new Thread(ConnectWorkerThread);t.Name = &quot;Connection to &quot; + Hostname;t.IsBackground = true;t.Start(connectTask); ConnectWorkerThread线程，调用NewSession方法创建连接的会话。12345678910111213private void ConnectWorkerThread(object o) &#123; ConnectTask task = (ConnectTask)o; Exception error = null; Pool pool = null; try &#123; log.DebugFormat(&quot;IXenConnection: trying to connect to &#123;0&#125;&quot;, HostnameWithPort); //创建连接会话 Session session = NewSession(task.Hostname, task.Port, Username, Password, false); // Save the session so we can log it out later task.Session = session; NewSession方法调用GetNewSession方法1234567private Session NewSession(string hostname, int port, string username, string password, bool isElevated) &#123; Password = password; Username = username; return GetNewSession(hostname, port, username, password, isElevated); &#125; GetNewSession方法，创建会话之后，使用用户名密码登录1234567891011121314151617181920212223private Session GetNewSession(string hostname, int port, string username, string password, bool isElevated) &#123; const int DELAY = 250; // unit = ms int attempt = 0; while (true) &#123; attempt++; string uname = isElevated ? username : Username; string pwd = isElevated ? password : Password; // Keep the password that we&apos;re using for this iteration, as it may // be changed by another thread handling an authentication failure. // For elevated session we use the elevated username and password passed into this function, // as the connection&apos;s Username and Password are not updated. //创建会话 Session session = SessionFactory.CreateSession(this, hostname, port); if (isElevated) session.IsElevatedSession = true; try &#123; //使用用户名、密码登录 session.login_with_password(uname, pwd, !string.IsNullOrEmpty(Version) ? Version : Helper.APIVersionString(API_Version.LATEST), Session.UserAgent); return session; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[XenCenter分析（一）]]></title>
      <url>%2F2017%2F02%2F27%2Fxencenter-1%2F</url>
      <content type="text"><![CDATA[XenCenter是XenServer的Windows客户端管理工具，可以非常方便地管理和监视XenServer主机和资源池，以及部署、监视、管理和迁移虚拟机。 主要有以下功能： 资源池管理 物理主机管理 虚拟机管理 存储池管理 模板管理 代码放在GitHub上托管，XenCenter 这个是我修改过的XenCenter版本。 项目结构 CFUValidator： CommandLib：命令操作的公共类库，主要是IO、tar、HTTP splash：启动界面的效果，使用C++编写 XenAdmin：XenCenter的主项目，包含所有的page、dialog、view XenAdminTests：XenCenter的测试代码 XenCenterLib：一些工具类库 XenCenterVNC：物理主机、虚拟机的控制台 XenModel：模型类以及XAPI接口 XenOvfApi：OVF相关的API XenOvfTransport：OVF导入导出的工具类库 XenServerHealthCheck：主机健康状态检查 xva_verify：xva文件校验 XenAdminXenAdmin是XenCenter代码的主程序的入口，基于C#的WinForm应用程序开发。研究XenCenter先从XenAdmin项目入手。 XenAdmin主要有以下的代码目录： Actions：界面UI与导入OVF虚拟机的action类 Alerts：告警提示类 Commands：命令模式Commands操作类 ConsoleView：VNC控制台的视图与逻辑操作类 Controls：UI界面 Core：公共类库 Diagnostics：资源（Pool、Host、VM、SR等）检查、状态（HA、PBD等）检查 Dialogs：UI的所有弹出对话框 Help：帮助页面以及文档 HomePage：启动XenCenter之后的主页信息 Images：图片资源 Network：SSL连接与XAPI连接XenServer Plugins：插件管理 Properties：license、配置、资源 RDP： ReportViewer：报表视图 ServerDBs：模拟数据库 SettingsPanels：资源设置相关的page TabPages：菜单的tabpages TestResources：测试资源 Utils：工具类 Wizards：向导基础page与向导式操作page Wlb：负载均衡 XenSearch：搜索相关的类 Program.csWinForm应用的Program类main方法12345678static public void Main(string[] Args) &#123; //Upgrade settings //获取当前操作系统信息 System.Reflection.Assembly a = System.Reflection.Assembly.GetExecutingAssembly(); Version appVersion = a.GetName().Version; string appVersionString = appVersion.ToString(); log.DebugFormat(&quot;Application version of new settings &#123;0&#125;&quot;, appVersionString); 1234567//清空XAPI连接信息ConnectionsManager.XenConnections.Clear();//清空连接的历史信息ConnectionsManager.History.Clear();//初始化搜索Search.InitSearch(Branding.Search);TreeSearch.InitSearch(); 1234567891011121314151617switch (Environment.OSVersion.Version.Major) &#123; case 6: // Vista, 2K8, Win7. if (Application.RenderWithVisualStyles) &#123; // Vista, Win7 with styles. //根据操作系统版本设置样式 TitleBarStartColor = Color.FromArgb(242, 242, 242); TitleBarEndColor = Color.FromArgb(207, 207, 207); TitleBarBorderColor = Color.FromArgb(160, 160, 160); TitleBarForeColor = Color.FromArgb(60, 60, 60); HeaderGradientForeColor = Color.White; HeaderGradientFont = new Font(DefaultFont.FontFamily, 11.25f); HeaderGradientFontSmall = DefaultFont; TabbedDialogHeaderFont = HeaderGradientFont; TabPageRowBorder = Color.Gainsboro; TabPageRowHeader = Color.WhiteSmoke; 1234//运行主窗口MainWindow mainWindow = new MainWindow(argType, args);Application.Run(mainWindow); MainWindow是XenCenter的主窗口。 上面部分是菜单栏与操作栏 左侧是树形菜单栏 右侧是主页与选项]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WinCenterClient分析（二）]]></title>
      <url>%2F2017%2F02%2F24%2Fwincenterclient-2%2F</url>
      <content type="text"><![CDATA[WinCenterClient是WinCenter-Appliance虚拟化管理系统安装程序，使用C# WinForm 框架Wizard方式来实现导入WinCenter虚拟机的功能。 包含以下项目 SharpSSH：C#版本的连接SSH类库，版本比较旧，一直未更新，在WinCenterClient5.5及以后的版本不再使用 Renci.SshNet：C#版本的连接SSH类库，在WinCenterClient5.4及以后的版本使用 WinCenter：WinForm界面 WinServer：C#版本的API Winzardlib：向导式框架类库 SharpSSHSharpSSH是一个C#开发的实现了SSH2协议的开源组件，支持SSH/SCP/SFTP等协议。 SharpSSH执行sh命令的输入输出都是定向到console。因此不容易从其中取出它的结果。因此需要对源码进行一定的修改，从而得到我们想要的结果。 SSH操作： SharpSSH中修改 SshStream.cs，增加一个方法，把输出定向到流中public void set_OutputStream(Stream stream) WinCenter中定义ShellHelp.cs工具类来连接SSH，执行Linux命令，获取输出结果字符串 由于SharpSSH版本比较旧，一直未更新，而且不支持一些加密算法，在WinCenterClient5.4以后的版本中使用Renci.SshNet来代替 Renci.SshNetRenci.SshNet支持多种加密算法，且不需要处理输入输出流，可以代替SharpSSH。 WinServerWinServer是C#版本的API。 WizardLibWizardLib是Wizard向导式WinForm 界面的lib类库。 主要的类： WizardFormWizardForm派生自Form，它负责管理WizardPage集合。另外，它提供了缺省的Wizard界面。创建你自己的向导类的第一件事就是扩展这个类，通常从该类继承，编辑属性或者添加额外的按钮。 WizardPageWizardPage派生自UserControl。提供了向导相关的方法，如 OnSetActive, OnWizardNext等。 WizardLib类图 WizardLib通用组件提供复用的组件给WinCenter界面使用。 IP地址及网关输入控件IPBox IPBox继承UserControl，在组件中放置四个textBox，通过重写OnPaint 、OnResize方法绘制边框等。 Button控件MyButton 默认的Button控件在设置透明背景图片时，鼠标点下会出现黑色的边框。MyButton控件继承Button，MyButton重写ShowFocusCues方法，去掉黑色的边框。 Loading效果控件MyOpaqueLayer 在加载数据时间比较长的情况下，需要使用loading效果提示用户正在加载数据或者操作。 MyOpaqueLayer继承Control，重写OnPaint方法绘制窗体，设置透明的屏蔽层。 OpaqueCommand类用来显示与隐藏屏蔽层。 DataGridView单选框控件 要达到单选效果，需要在业务处理中监听单元格点击事件，触发时，选择当前行，并取消其他所有行的选中，同时更改数据状态。 DataGridView组件默认是不支持单选，要实现单选的功能必须重写DataGridViewCell DataGridViewDisableCheckBoxCell继承DataGridViewCheckBoxCell，重写Paint方法来绘制单选框RadioButton DataGridViewDisableCheckBoxColumn 继承DataGridViewCheckBoxColumn]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WinCenterClient分析（一）]]></title>
      <url>%2F2017%2F02%2F24%2Fwincenterclient-1%2F</url>
      <content type="text"><![CDATA[WinCenterClient是WinCenter-Appliance虚拟化管理系统安装程序，使用C# WinForm 框架Wizard方式来实现导入WinCenter虚拟机的功能。 实现过程 使用HHTP/HHTPS方式导入xva模板文件到主机上，并生成虚拟机 根据元数据文件信息给虚拟机配置相应资源 配置虚拟机 删除原有的VIF 创建新的VIF，生成新的MAC地址，并设置网络为目标主机的管理网络 设置虚拟机的名称 设置虚拟机所属的物理主机 启动虚拟机 动态配置虚拟机的网络信息（IP、网关、子网掩码） 向导式操作步骤向导式的操作，主要由以下步骤组成： 导入来源：虚拟机模板的文件路径 配置主机：虚拟机导入的目标主机 配置存储：虚拟机磁盘数据导入的目标存储 配置网络：配置虚拟机的IP、子网掩码、网关信息 安装虚拟机：导入、配置并启动虚拟机 导入来源选择提前制作好的xva模板文件作为导入来源。在这个步骤中校验模板文件并获取模板文件的元数据信息，包括虚拟磁盘大小、虚拟CPU数量、内存大小、VIF数量、虚拟化版本信息等 配置主机配置虚拟机导入的目标主机 配置存储配置虚拟机导入的目标存储，目标存储可以是目标主机上的本地存储，也可以是资源池内的共享存储 配置网络配置虚拟机可用的IP、子网掩码、网关信息 安装虚拟机导入、配置并启动虚拟机，这个步骤是整个安装过程的最重要的步骤。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo使用本地图片]]></title>
      <url>%2F2017%2F02%2F24%2Fhexo-local-pic%2F</url>
      <content type="text"><![CDATA[在文章中使用本地方式引用图片。 安装插件npm install hexo-asset-image --save 修改站点_config.ymlpost_asset_folder:true Hexo提供了一种更方便管理 Asset 的设定：post_asset_folder。当设置post_asset_folder为true参数后，在hexo-hey中发布文章时，Hexo会自动建立一个与文章同名的文件夹，可以把与该文章相关的所有图片资源都放到那个文件夹。 hexo使用本地图片 ├── 图片名称.png ├── 图片名称1.png └── 图片名称2.png hexo使用本地图片.md 在文章中直接引用图片![图片title](图片名称.png)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo博客中的next主题配置]]></title>
      <url>%2F2017%2F02%2F15%2Fhexo-config-more%2F</url>
      <content type="text"><![CDATA[博客搭建之后，修改next主题，增加以下功能： 首页「阅读全文」 站内搜索 文章阅读次数 文章多说评论 社交链接GitHub 设置「阅读全文」在首页显示文章的摘录并显示“阅读全文”按钮，可以通过以下方法： 在每篇文章内容中添加 &lt;!– more –&gt; 配置站内搜索安装hexo-generator-searchdb 1$ npm install hexo-generator-searchdb --save 在站点的 _config.yml中增加 search: path: search.xml field: post 配置阅读次数注册LeanCloud账号，新建应用和Class，可参考next主题配置阅读次数统计 修改next主题的_config.yml文件，将LeanCloud的App ID与App Key复制到leancloud_visitors部分 leancloud_visitors: enable: true app_id: LE6b1aXadasdaksderewldfgrn-gzGzoHsz app_key: WHQseHdLj7t7hyj5546546EXO7 配置多说评论注册多说，并创建站点，可参考next主题集成第三方服务 修改站点与next主题的_config.yml文件，配置duoshuo_shortname duoshuo_shortname: hl10502 hl10502为多说站点名称 修改next主题的_config.yml文件，配置多说分享 uoshuo_share: true 在每一条多说评论后显示评论者所使用的代理信息（如 操作系统、浏览器），修改next主题_config.yml文件，配置 duoshuo_info字段， 设置如下： duoshuo_info: ua_enable: true admin_enable: false user_id: 0 #admin_nickname: 侧边栏社交链接侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。两者配置均在next主题的_config.yml文件中。 配置链接，修改social social: #LinkLabel: Link GitHub: https://github.com/hl10502 #Twitter: https://twitter.com/your-user-name #微博: http://weibo.com/your-user-name #豆瓣: http://douban.com/people/your-user-name #知乎: 配置链接图标，修改 social_icons: enable: true # Icon Mappings. # KeyMapsToSocalItemKey: NameOfTheIconFromFontAwesome GitHub: github #Twitter: twitter #Weibo: weibo]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo配置标签]]></title>
      <url>%2F2017%2F02%2F14%2Fhexo-config-tags%2F</url>
      <content type="text"><![CDATA[创建标签页面在source目录下，自动创建tags/index.md文件1$ hexo new page tags 修改标签内容添加以下内容到source/tags/index.md文件 type: &quot;tags&quot; comments: false 修改_config.yml站点的_config.yml文件配置tag_dir tag_dir: tags next主题的_config.yml文件配置tags menu: tags: /tags menu_icons: tags: tags 博客文章中添加tags比如在博客文章《使用hexo+github搭建个人博客》的title下面添加 tags: [hexo,hexo-hey,github,个人博客]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用hexo+github搭建个人博客]]></title>
      <url>%2F2017%2F02%2F13%2Fhexo-github-setup-blog%2F</url>
      <content type="text"><![CDATA[使用hexo+github在windows7环境下搭建个人技术博客,在git bash下执行以下bash操作命令。 环境准备 windows7 x64 nodejs 6.9.5 git 2.11.0 hexo 3.2.2 配置github创建仓库打开github主页，新建github仓库，仓库名称为”hl10502.github.com” 配置SSH key生成key 1$ ssh-keygen -t rsa -C "xxx@126.com" //邮箱为github注册的邮箱 打开github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key： 将生成的key文件（用户目录下的.ssh\id_rsa.pub）内容copy到新建的key中 测试SSH配置 1$ ssh -T git@github.com //固定邮箱 配置全局的用户名、邮箱 1$ git config --global user.name "hl10502" //github用户名 1$ git config --global user.email "xxx@126.com" //邮箱为github注册的邮箱 配置hexo安装hexo（注意：之前安装的nodejs在安装时需要添加环境变量） 1$ npm install -g hexo 在F盘下创建 F:\blog\hexo文件夹，作为blog代码目录，初始化hexo 1$ cd /f/blog/hexo 1$ hexo init 下载next主题 1$ git clone https://github.com/iissnan/hexo-theme-next themes/next 修改站点的_config.yml文件的主题为 theme: next 生成静态文件 1$ hexo g 启动hexo服务，可以通过 http://localhost:4000 访问 1$ hexo s 配置hexo-hey插件安装hexo-hey插件 1$ npm install hexo-hey --save 配置站点的_config.yml文件，添加admin部分 #hexo-hey插件 admin: name: hexo password: hey secret: hey hexo expire: 60*1 # cors: http://localhost:4000 登录 http://localhost:4000/admin，可以新建文章，发布到本地生成.md文件 用户名：hexo密码：hey 发布hexo到github安装hexo-deployer-git插件 1$ cd /f/blog/hexo/ 1$ npm install hexo-deployer-git --save 配置站点的_config.yml文件，修改deploy部分，使用SSH上传文件 deploy: type: git repository: git@github.com:hl10502/hl10502.github.com.git branch: master hl10502为github的用户名，hl10502.github.com为仓库名称 部署hexo，上传到github 1$ hexo d 访问个人博客： https://hl10502.github.io]]></content>
    </entry>

    
  
  
</search>
